{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to create dense neural networks using Keras.\n",
    "    2. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    3. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    \n",
    "To save time we will train each experiment only for 50 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "**HINT: YOU CAN HIT SHIFT-ENTER TO RUN EACH CELL. NOTE THAT IF A CELL IS DEPENDENT ON A PREVIOUS CELL, YOU WILL NEED TO RUN THE PREVIOUS CELL(S) FIRST **\n",
    "\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 3, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 3 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Keras provides the to_categorical function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "Y = to_categorical(y = iris.target, num_classes = 3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, \n",
    "                                                    random_state = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create the neural network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "nn.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd = SGD(lr = 0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "          metrics = 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 50 epochs. We will shuffle the training data between epochs, and provide validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 0.1076 - accuracy: 0.9667 - val_loss: 0.1281 - val_accuracy: 0.9667\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.1093 - accuracy: 0.9667 - val_loss: 0.1108 - val_accuracy: 0.9667\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1029 - accuracy: 0.9667 - val_loss: 0.0949 - val_accuracy: 0.9667\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1054 - accuracy: 0.9667 - val_loss: 0.0970 - val_accuracy: 0.9667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.1026 - accuracy: 0.9750 - val_loss: 0.1063 - val_accuracy: 0.9667\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1036 - accuracy: 0.9667 - val_loss: 0.1010 - val_accuracy: 0.9667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1029 - accuracy: 0.9667 - val_loss: 0.0773 - val_accuracy: 0.9667\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.1044 - accuracy: 0.9667 - val_loss: 0.0862 - val_accuracy: 0.9667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1071 - accuracy: 0.9750 - val_loss: 0.1059 - val_accuracy: 0.9667\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.1024 - accuracy: 0.9583 - val_loss: 0.0957 - val_accuracy: 0.9667\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1007 - accuracy: 0.9750 - val_loss: 0.0988 - val_accuracy: 0.9667\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 0.1032 - accuracy: 0.9667 - val_loss: 0.0914 - val_accuracy: 0.9667\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.1022 - accuracy: 0.9750 - val_loss: 0.0822 - val_accuracy: 0.9667\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.1002 - accuracy: 0.9750 - val_loss: 0.0931 - val_accuracy: 0.9667\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.1001 - accuracy: 0.9750 - val_loss: 0.0963 - val_accuracy: 0.9667\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.1005 - accuracy: 0.9667 - val_loss: 0.0861 - val_accuracy: 0.9667\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.0990 - accuracy: 0.9750 - val_loss: 0.0964 - val_accuracy: 0.9667\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 36ms/step - loss: 0.0996 - accuracy: 0.9667 - val_loss: 0.0804 - val_accuracy: 0.9667\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0980 - accuracy: 0.9750 - val_loss: 0.0857 - val_accuracy: 0.9667\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0966 - accuracy: 0.9750 - val_loss: 0.0921 - val_accuracy: 0.9667\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.0974 - accuracy: 0.9667 - val_loss: 0.0991 - val_accuracy: 0.9667\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0996 - accuracy: 0.9583 - val_loss: 0.0908 - val_accuracy: 0.9667\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0957 - accuracy: 0.9667 - val_loss: 0.0802 - val_accuracy: 0.9667\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0977 - accuracy: 0.9750 - val_loss: 0.0764 - val_accuracy: 0.9667\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0963 - accuracy: 0.9750 - val_loss: 0.0818 - val_accuracy: 0.9667\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.0956 - accuracy: 0.9750 - val_loss: 0.0787 - val_accuracy: 0.9667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.0969 - accuracy: 0.9667 - val_loss: 0.0741 - val_accuracy: 0.9667\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0967 - accuracy: 0.9750 - val_loss: 0.0795 - val_accuracy: 0.9667\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.0954 - accuracy: 0.9750 - val_loss: 0.0837 - val_accuracy: 0.9667\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0941 - accuracy: 0.9667 - val_loss: 0.0818 - val_accuracy: 0.9667\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0938 - accuracy: 0.9750 - val_loss: 0.0835 - val_accuracy: 0.9667\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.0948 - accuracy: 0.9583 - val_loss: 0.0749 - val_accuracy: 0.9667\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.0932 - accuracy: 0.9750 - val_loss: 0.0869 - val_accuracy: 0.9667\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.0937 - accuracy: 0.9750 - val_loss: 0.0762 - val_accuracy: 0.9667\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 132ms/step - loss: 0.0948 - accuracy: 0.9750 - val_loss: 0.0885 - val_accuracy: 0.9667\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.0939 - accuracy: 0.9667 - val_loss: 0.0805 - val_accuracy: 0.9667\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.0933 - accuracy: 0.9667 - val_loss: 0.0714 - val_accuracy: 0.9667\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0925 - accuracy: 0.9750 - val_loss: 0.0904 - val_accuracy: 0.9667\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.0912 - accuracy: 0.9667 - val_loss: 0.0822 - val_accuracy: 0.9667\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.0927 - accuracy: 0.9667 - val_loss: 0.0760 - val_accuracy: 0.9667\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0913 - accuracy: 0.9750 - val_loss: 0.0781 - val_accuracy: 0.9667\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.0915 - accuracy: 0.9667 - val_loss: 0.0719 - val_accuracy: 0.9667\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0982 - accuracy: 0.9667 - val_loss: 0.0916 - val_accuracy: 0.9667\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0900 - accuracy: 0.9667 - val_loss: 0.0794 - val_accuracy: 0.9667\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.0939 - accuracy: 0.9667 - val_loss: 0.0691 - val_accuracy: 0.9667\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.0917 - accuracy: 0.9750 - val_loss: 0.0680 - val_accuracy: 0.9667\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.0903 - accuracy: 0.9750 - val_loss: 0.0792 - val_accuracy: 0.9667\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.0903 - accuracy: 0.9667 - val_loss: 0.0745 - val_accuracy: 0.9667\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.0897 - accuracy: 0.9750 - val_loss: 0.0884 - val_accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.0926 - accuracy: 0.9750 - val_loss: 0.0905 - val_accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb66f57950>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, Y_train, shuffle = True, epochs = 50, \n",
    "      validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 1s 95ms/step - loss: 112019258510684991782912.0000 - accuracy: 0.4167 - val_loss: 17600.1367 - val_accuracy: 0.3667\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 1600028288.0000 - accuracy: 0.3833 - val_loss: 36532.6367 - val_accuracy: 0.2000\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 29165.0254 - accuracy: 0.3917 - val_loss: 26181797675008.0000 - val_accuracy: 0.4333\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 9282576711680.0000 - accuracy: 0.3750 - val_loss: 41220.1445 - val_accuracy: 0.2000\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 39815.1133 - accuracy: 0.2833 - val_loss: 26694.3691 - val_accuracy: 0.3667\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 31279.7461 - accuracy: 0.3583 - val_loss: 28303.4785 - val_accuracy: 0.2000\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 26404.4473 - accuracy: 0.4167 - val_loss: 28868.5410 - val_accuracy: 0.4333\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 38329.1680 - accuracy: 0.2917 - val_loss: 35340.2031 - val_accuracy: 0.3667\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 29118.2871 - accuracy: 0.3500 - val_loss: 73060.4219 - val_accuracy: 0.2000\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 35197.8516 - accuracy: 0.3833 - val_loss: 10768.7588 - val_accuracy: 0.2000\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 31568.5879 - accuracy: 0.3417 - val_loss: 15812.9854 - val_accuracy: 0.4333\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 34561.8008 - accuracy: 0.3083 - val_loss: 50305.4805 - val_accuracy: 0.3667\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 33617.4414 - accuracy: 0.3417 - val_loss: 65109.0430 - val_accuracy: 0.2000\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 45848.1016 - accuracy: 0.2583 - val_loss: 3534.6521 - val_accuracy: 0.3667\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 24832.4727 - accuracy: 0.3833 - val_loss: 23117.9883 - val_accuracy: 0.3667\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 31757.1777 - accuracy: 0.3667 - val_loss: 39840.7578 - val_accuracy: 0.4333\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 36757.9844 - accuracy: 0.3167 - val_loss: 49449.3125 - val_accuracy: 0.2000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 34ms/step - loss: 30986.9883 - accuracy: 0.3917 - val_loss: 17770.7617 - val_accuracy: 0.3667\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 29265.8613 - accuracy: 0.3500 - val_loss: 43025.7031 - val_accuracy: 0.2000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 32540.8066 - accuracy: 0.3667 - val_loss: 27583.8164 - val_accuracy: 0.4333\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 34431.5938 - accuracy: 0.3250 - val_loss: 35490.9805 - val_accuracy: 0.2000\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 36325.5352 - accuracy: 0.3083 - val_loss: 36590.2031 - val_accuracy: 0.3667\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 32295.3730 - accuracy: 0.3583 - val_loss: 24935.4316 - val_accuracy: 0.2000\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 25526.9160 - accuracy: 0.4167 - val_loss: 27375.4824 - val_accuracy: 0.4333\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 38ms/step - loss: 33363.8867 - accuracy: 0.3500 - val_loss: 31277.7051 - val_accuracy: 0.3667\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 29693.5840 - accuracy: 0.3417 - val_loss: 53083.2578 - val_accuracy: 0.3667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 41453.3633 - accuracy: 0.3167 - val_loss: 7201.8745 - val_accuracy: 0.4333\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 29381.0840 - accuracy: 0.3333 - val_loss: 19389.3750 - val_accuracy: 0.4333\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 32399.5391 - accuracy: 0.3417 - val_loss: 51507.4297 - val_accuracy: 0.4333\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 38919.4414 - accuracy: 0.3167 - val_loss: 49900.6992 - val_accuracy: 0.2000\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 40535.6016 - accuracy: 0.2917 - val_loss: 3799.0942 - val_accuracy: 0.4333\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 33580.7305 - accuracy: 0.2667 - val_loss: 50825.7422 - val_accuracy: 0.3667\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 34088.5742 - accuracy: 0.3833 - val_loss: 46611.7070 - val_accuracy: 0.4333\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 38060.0391 - accuracy: 0.3417 - val_loss: 26811.2285 - val_accuracy: 0.2000\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 42349.9414 - accuracy: 0.2417 - val_loss: 21068.8008 - val_accuracy: 0.3667\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 31045.2090 - accuracy: 0.3583 - val_loss: 27757.5449 - val_accuracy: 0.4333\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 35820.4570 - accuracy: 0.2833 - val_loss: 79866.7812 - val_accuracy: 0.2000\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 43117.2969 - accuracy: 0.3333 - val_loss: 16082.0508 - val_accuracy: 0.2000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 35240.5703 - accuracy: 0.2917 - val_loss: 33568.8047 - val_accuracy: 0.3667\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 34759.6055 - accuracy: 0.3417 - val_loss: 23000.5977 - val_accuracy: 0.4333\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 31369.0391 - accuracy: 0.3417 - val_loss: 34840.8828 - val_accuracy: 0.4333\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 34830.8789 - accuracy: 0.3500 - val_loss: 34159.0742 - val_accuracy: 0.3667\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 29621.5977 - accuracy: 0.3583 - val_loss: 52713.9961 - val_accuracy: 0.2000\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 38262.2852 - accuracy: 0.3167 - val_loss: 12445.0537 - val_accuracy: 0.4333\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 30622.5117 - accuracy: 0.3167 - val_loss: 29493.6621 - val_accuracy: 0.4333\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 45047.8867 - accuracy: 0.2500 - val_loss: 51624.3555 - val_accuracy: 0.3667\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 44291.7344 - accuracy: 0.2583 - val_loss: 35908.4453 - val_accuracy: 0.2000\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 37211.0547 - accuracy: 0.3250 - val_loss: 6021.4375 - val_accuracy: 0.4333\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 31228.4434 - accuracy: 0.3000 - val_loss: 50096.5742 - val_accuracy: 0.3667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 35294.2852 - accuracy: 0.3500 - val_loss: 47236.7148 - val_accuracy: 0.4333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb67659ad0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create the neural network, playing with learning rates\n",
    "# nn = Sequential()\n",
    "# nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "# nn.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# # Create our optimizer\n",
    "# sgd = SGD(lr = 100000)\n",
    "\n",
    "# # 'Compile' the network to associate it with a loss function,\n",
    "# # an optimizer, and what metrics we want to track\n",
    "# nn.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "#           metrics = 'accuracy')\n",
    "\n",
    "# nn.fit(X_train, Y_train, shuffle = True, epochs = 50,\n",
    "#       validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the neural network - Testing Momentum\n",
    "# nn = Sequential()\n",
    "# nn.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "# nn.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# # Create our optimizer\n",
    "# sgd = SGD(lr = 0.1,momentum=1)\n",
    "\n",
    "# # 'Compile' the network to associate it with a loss function,\n",
    "# # an optimizer, and what metrics we want to track\n",
    "# nn.compile(loss='categorical_crossentropy', optimizer=sgd,\n",
    "#           metrics = 'accuracy')\n",
    "\n",
    "# nn.fit(X_train, Y_train, shuffle = True, epochs = 50,\n",
    "#       validation_data = (X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***\n",
    "\n",
    "\n",
    "**Answer: The model is Overfitting as the validation loss is not minimum for the last epoch. Val_loss for epoch 49 was 0.0884  which is lower than 0.0905, the val_loss of epcoh 50.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 4_\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://keras.io/api/optimizers/sgd/). What does the lr parameter do? ***(1 MARK)***\n",
    "\n",
    "**Answer: lr is learning rate, The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated.**\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The documentation states that the momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. ***(2 MARKS)***\n",
    "\n",
    "**Answer: Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way. The momentum parameter increases the velocity if we are moving towards lowest error rate and decreases if we are moving away from local minima, which helps us to reach local minima i.e. lower error rate. Oftenly with large learning rate, we could face the issues of oscillating away from local minima, this momentum parameter helps us avoid that and dampens the oscillations, hence makes it possible to achieve lowest error rate.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 3_\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and validation accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc.***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    |     0.6917          |    0.5667          |   Progresses steadily                |\n",
    "|0.1     |    0.9750           |   0.9667           |   Progresses steadily                |\n",
    "|1.0     |     0.9          |       1.0          |      Progresses              |\n",
    "|10.0    |     0.33          |       0.37          |    some oscillation               |\n",
    "|100     |     0.325          |       0.433          |    some oscillation               |\n",
    "|1000    |      0.3         |        0.433         |     some oscillation              |\n",
    "|10000   |      0.77         |       0.57          |     some oscillation              |\n",
    "| 100000 |       0.28        |      0.433           |   some oscillation                |\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***\n",
    "\n",
    "**Answer: Smaller learning rate showed Slower progress towards increase in accuracy, whereas moderate learning rates give very good accuracy. Large learning rates forced oscillations as we deviate from local minima, hence the lower accuracies and oscillations observed.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. Note: Do not use the Nesterov parameter - Leave it as False.\n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Validation Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |   0.9333            |     0.9333            |      steady progress             |\n",
    "|0.01      |    0.925           |       0.9667           |      steady progress             |\n",
    "|0.1       |    0.925           |      0.97           |      steady progress              |\n",
    "|1.0       |    0.925            |           1      |      steady progress              |\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***\n",
    "\n",
    "**Answer: Yes, The Validation accuracy has gradually improved with the increase in Momentum. Indicating betting learning as compared to default momentum value.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 Using Raw Unscaled Data\n",
    "\n",
    "We begin by using unscaled X and Y data. The code below will create 120 training samples and 30 testing samples (20% of the total of 150 samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unscaled = iris.data\n",
    "Y_raw = iris.target\n",
    "X_utrain, X_utest, Y_utrain, Y_utest = train_test_split(X_unscaled, Y,\n",
    "                                                        test_size = 0.2,\n",
    "                                                        random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "Create a new neural network called \"nn2\" below using a single hidden layer of 100 neurons. Train using the data in X_utrain, X_utest and validate with Y_utrain and Y_utest. Again use the SGD optimizer with a learning rate of 0.1 and no momentum, and train for 50 epochs. ***(3 marks)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 1s 139ms/step - loss: 1.2467 - accuracy: 0.4000 - val_loss: 2.1555 - val_accuracy: 0.5667\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1460 - accuracy: 0.5167 - val_loss: 0.9508 - val_accuracy: 0.5667\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.6736 - accuracy: 0.7000 - val_loss: 0.6021 - val_accuracy: 0.6667\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.5935 - accuracy: 0.7583 - val_loss: 0.6589 - val_accuracy: 0.5667\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.4915 - accuracy: 0.8583 - val_loss: 0.5936 - val_accuracy: 0.6000\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.5143 - accuracy: 0.7250 - val_loss: 0.7527 - val_accuracy: 0.5667\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.4787 - accuracy: 0.7667 - val_loss: 0.5907 - val_accuracy: 0.6000\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5299 - accuracy: 0.6833 - val_loss: 0.5560 - val_accuracy: 0.6000\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.3863 - accuracy: 0.8750 - val_loss: 0.4357 - val_accuracy: 0.7333\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3594 - accuracy: 0.9167 - val_loss: 0.3688 - val_accuracy: 0.8667\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4110 - accuracy: 0.7750 - val_loss: 0.5182 - val_accuracy: 0.8000\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3990 - accuracy: 0.8250 - val_loss: 0.4337 - val_accuracy: 0.7000\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.3164 - accuracy: 0.9500 - val_loss: 0.3586 - val_accuracy: 0.9000\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3526 - accuracy: 0.8500 - val_loss: 0.3302 - val_accuracy: 0.9667\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 28ms/step - loss: 0.3211 - accuracy: 0.8917 - val_loss: 0.4873 - val_accuracy: 0.8000\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5752 - accuracy: 0.7250 - val_loss: 1.1435 - val_accuracy: 0.5667\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.4134 - accuracy: 0.7750 - val_loss: 0.3398 - val_accuracy: 0.8000\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 30ms/step - loss: 0.3298 - accuracy: 0.8667 - val_loss: 0.3025 - val_accuracy: 0.9000\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.4473 - accuracy: 0.7333 - val_loss: 0.4891 - val_accuracy: 0.8000\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 0.4436 - accuracy: 0.7333 - val_loss: 0.6189 - val_accuracy: 0.6000\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2860 - accuracy: 0.8833 - val_loss: 0.2956 - val_accuracy: 0.9667\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2458 - accuracy: 0.9750 - val_loss: 0.2809 - val_accuracy: 0.9000\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2525 - accuracy: 0.9000 - val_loss: 0.2660 - val_accuracy: 0.9333\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.2475 - accuracy: 0.9500 - val_loss: 0.5844 - val_accuracy: 0.6000\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 42ms/step - loss: 0.4278 - accuracy: 0.7667 - val_loss: 0.3131 - val_accuracy: 0.8000\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.2458 - accuracy: 0.9250 - val_loss: 0.2938 - val_accuracy: 0.8667\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4361 - accuracy: 0.8167 - val_loss: 0.2536 - val_accuracy: 0.9667\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.2235 - accuracy: 0.9583 - val_loss: 0.2423 - val_accuracy: 0.9667\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2630 - accuracy: 0.9167 - val_loss: 0.2673 - val_accuracy: 0.8667\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2580 - accuracy: 0.8917 - val_loss: 0.2994 - val_accuracy: 0.8667\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 0.2010 - accuracy: 0.9500 - val_loss: 0.2965 - val_accuracy: 0.8000\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.2976 - accuracy: 0.8500 - val_loss: 0.3509 - val_accuracy: 0.8000\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2451 - accuracy: 0.8750 - val_loss: 0.4897 - val_accuracy: 0.7000\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 29ms/step - loss: 0.3437 - accuracy: 0.7833 - val_loss: 0.7461 - val_accuracy: 0.6000\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6194 - accuracy: 0.6833 - val_loss: 0.2460 - val_accuracy: 0.9333\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1949 - accuracy: 0.9583 - val_loss: 0.2203 - val_accuracy: 0.9667\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1741 - accuracy: 0.9667 - val_loss: 0.3187 - val_accuracy: 0.8000\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2668 - accuracy: 0.9000 - val_loss: 0.3257 - val_accuracy: 0.8000\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 0.1688 - accuracy: 0.9667 - val_loss: 0.2029 - val_accuracy: 0.9667\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.1901 - accuracy: 0.9417 - val_loss: 0.4690 - val_accuracy: 0.7333\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.2138 - accuracy: 0.9167 - val_loss: 0.8456 - val_accuracy: 0.6000\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.2692 - accuracy: 0.8667 - val_loss: 0.5227 - val_accuracy: 0.8000\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 44ms/step - loss: 0.3774 - accuracy: 0.8167 - val_loss: 0.1940 - val_accuracy: 0.9667\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.1542 - accuracy: 0.9667 - val_loss: 0.3125 - val_accuracy: 0.8000\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.1906 - accuracy: 0.9417 - val_loss: 0.3237 - val_accuracy: 0.8000\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5006 - accuracy: 0.7917 - val_loss: 0.8696 - val_accuracy: 0.6000\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3046 - accuracy: 0.8583 - val_loss: 0.6103 - val_accuracy: 0.6667\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1709 - accuracy: 0.9417 - val_loss: 0.2210 - val_accuracy: 0.9333\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 32ms/step - loss: 0.1330 - accuracy: 0.9750 - val_loss: 0.1738 - val_accuracy: 0.9667\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 27ms/step - loss: 0.1755 - accuracy: 0.9250 - val_loss: 0.5623 - val_accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb4be8b050>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enter your code for Question 5 below. To TA: 3 marks for code.\n",
    "\"\"\"\n",
    "# Create the neural network\n",
    "nn2 = Sequential()\n",
    "nn2.add(Dense(100, input_shape = (4, ), activation = 'relu'))\n",
    "nn2.add(Dense(3, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd2 = SGD(lr = 0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn2.compile(loss='categorical_crossentropy', optimizer=sgd2,\n",
    "          metrics = 'accuracy')\n",
    "\n",
    "nn2.fit(X_utrain, Y_utrain, shuffle = True, epochs = 50,\n",
    "      validation_data = (X_utest, Y_utest))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Question 5 continues)**\n",
    "\n",
    "Observe the training and validation error. Does not scaling the input affect the training? Why do you think this is so? What is the advantage of scaling? ***(5 MARKS)***\n",
    "\n",
    "**Answer:Both the Training and validation errors for unscaled data are greater than what we have for scaled data. The accuracies are low as well. In the case of Neural Networks Algorithms, feature scaling benefits optimization by: It makes the training faster. It prevents the optimization from getting stuck in local optima. It gives a better error surface shape.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 8_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "055b62fcaec9821674a26809055da6bc29fe87d96b4c426e8bdfbe57b9f21334"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
