{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report for the WeRateDogs Project\n",
    "\n",
    "## Temitope Adeosun\n",
    "\n",
    "### 19 October 2022\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling WeRateDogs twitter handle information clad to be a fairly difficult and time overwhelming task as against the impression i used to be below before I had started it. I do not solely ought to implement what I had learned during this amount however I conjointly developed Associate in Nursing intuition concerning the assorted functions that may be combined along to perform what might sound as extremely cumbersome or unrealizable tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For starters I solely had the unclean twitter_archives dataset accessible to be without delay downloaded manually. thus I did that 1st then I had to programmatically transfer the image_predictions tsv file from the uniform resource locator that was provided. The requests library came in handy for this terribly purpose. i used to be ready to access all the file content by mistreatment the get perform on the uniform resource locator then write that content to Associate in Nursing empty file. Thus, similar to that I had 2 files currently. however the foremost difficult task was gathering and writing the info for the third file. {the information|the info|the information} of interest was the WeRateDogs tweet data that I had to question the twitter API for. For that, 1st I required authorisation to use it. i used to be therefore needed to line up Associate in Nursing application Associate in Nursingd link it to my twitter account to urge distinctive keys that I may successively use to form an API object. This object was conjointly created with the assistance of tweepy, Associate in Nursing access library for the twitter API. And on mistreatment the library's get_status perform on every and each tweet id, i used to be ready to get every tweet's content. I reborn this content to the json format 1st for future use then wrote it to a computer file referred to as tweet_json. Doing all this for each tweet needed Maine to use a 'for' loop. each tweet's json object within the computer file was then scan one by one into Associate in Nursing empty list. the only purpose for that was to access all the json objects to extract the favorite and retweet counts for every and each tweet. once extracting all the counts I used them and their corresponding tweet ids to form a pandas dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I had three datasets gathered in total that I had to assess for quality and tidiness problems and clean. beyond any doubt the twitter archives table had most of the problems. This was as a result of it had been the biggest of the three tables and had a great deal of options concerning the tweets like tweet text, date and time of the tweet, etc. a great deal of the problems may simply be noticed by the oculus due to pandas functions like head, tail and sample. Others needed a bit additional analysis, in the main through summaries or filtering out sure sections of the info and evaluating the options. the information and value_counts functions were ofttimes used for an equivalent. Most of the tidiness problems concerned connexion of the tables and melting sure options into one column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final half was the foremost code destined half that was improvement. for every operation, I used the outline, code and check format whereby I 1st outlined the operation I wished to perform, then wrote the code for it and at last conducted tests on the dataset to check if that specific operation was performed. varied pandas functions were used, some in isolation and a few along. a great deal of the operations concerned extracting information from sure options and exchange the incorrect or incomplete column information therewith information. a great deal of unwanted rows and columns particularly those with retweets were removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all this project got Maine to figure with all the issues that accompany making your own datasets, checking out the assorted quality and tidiness connected problems then improvement up those problems to form the datasets analysis worthy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
