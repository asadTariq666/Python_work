{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Wrangle Report for the WeRateDogs Project</h1></center>\n",
    "<center><h2>Temitope Adeosun</h2></center>\n",
    "<center><h3>19 October 2022</h2></center>\n",
    "\n",
    "Wrangling WeRateDogs twitter handle data turned out to be a pretty challenging and time consuming task as opposed to the impression I was under before I had started it. I not only got to implement what I had learned in this period but I also developed an intuition about the various functions that can be combined together to perform what might seem as really cumbersome or undoable tasks.\n",
    "\n",
    "For starters I only had the unclean twitter_archives dataset available to be readily downloaded manually. So I did that first and then I had to programmatically download the image_predictions tsv file from the url which was provided. The requests library came in handy for this very purpose. I was able to access all the file content by using the get function on the url and then write that content to an empty file. Thus, just like that I had two files now. But the most challenging task was gathering and writing the data for the third file. The data of interest was the WeRateDogs tweet data which I had to query the twitter API for. For that, first I needed authorisation to use it. I was hence required to set up an application and link it to my twitter account to get unique keys which I could in turn use to create an API object. This object was also created with the help of tweepy, an access library for the twitter API. And on using the library's get_status function on each and every tweet id, I was able to get each tweet's content. I converted this content to the json format first for future use and then wrote it to a text file called tweet_json. Doing all this for every tweet required me to use a 'for' loop. Every tweet's json object in the text file was then read one by one into an empty list. The sole purpose for that was to access all the json objects to extract the favourite and retweet counts for each and every tweet. After extracting all the counts I used them and their corresponding tweet ids to create a pandas dataframe. \n",
    "\n",
    "Now I had 3 datasets gathered in total which I had to assess for quality and tidiness issues and clean. Undoubtedly the twitter archives table had most of the issues. This was because it was the largest of the 3 tables and had a lot of features regarding the tweets like tweet text, date and time of the tweet, etc. A lot of the issues could just be spotted by the naked eye thanks to pandas functions like head, tail and sample. Others required a little more analysis, mainly through summaries or filtering out certain sections of the data and evaluating the features. The info and value_counts functions were frequently used for the same. Most of the tidiness issues involved joining of the tables and melting certain features into a single column.\n",
    "\n",
    "The final part was the most code oriented part which was cleaning. For each operation, I used the define, code and test format wherein I first defined the operation I wanted to perform, then wrote the code for it and finally conducted tests on the dataset to see if that particular operation was performed. Various pandas functions were used, some in isolation and some together. A lot of the operations involved extracting data from certain features and replacing the inaccurate or incomplete column data with that data. A lot of unwanted rows and columns especially the ones with retweets were removed.\n",
    "\n",
    "All in all this project got me to work with all the problems that come with creating your own datasets, finding out the various quality and tidiness related issues and then cleaning up those issues to make the datasets analysis worthy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
