{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "We will achieve the following objectives in this lab:\n",
    "\n",
    "    1. An understanding of the practical limitations of using dense networks in complex tasks\n",
    "    2. Hands-on experience in building a deep learning neural network to solve a relatively complex task.\n",
    "    \n",
    "\n",
    "Each step may take a long time to run. You and your partner may want to work out how to do things simultaneously, but please do not miss out on any learning opportunities.\n",
    "\n",
    "\n",
    "## 2. Submission Instructions\n",
    "\n",
    "Please work together as a team of 2 to complete this lab. You will need to submit ONE copy of this notebook per team, but please fill in the names of both team members above. This lab is worth 55 marks.\n",
    "\n",
    "**DO NOT SUBMIT MORE THAN ONE COPY OF THIS LAB!**\n",
    "\n",
    "## 3. Creating a Dense Network for CIFAR-10\n",
    "\n",
    "We will now begin building a neural network for the CIFAR-10 dataset. The CIFAR-10 dataset consists of 50,000 32x32x3 (32x32 pixels, RGB channels) training images and 10,000 testing images (also 32x32x3), divided into the following 10 categories:\n",
    "\n",
    "    1. Airplane\n",
    "    2. Automobile\n",
    "    3. Bird\n",
    "    4. Cat\n",
    "    5. Deer\n",
    "    6. Dog\n",
    "    7. Frog\n",
    "    8. Horse\n",
    "    9. Ship\n",
    "    10. Truck\n",
    "    \n",
    "In the first two parts of this lab we will create a classifier for the CIFAR-10 dataset.\n",
    "\n",
    "### 3.1 Loading the Dataset\n",
    "\n",
    "We begin firstly by creating a Dense neural network for CIFAR-10. The code below shows how we load the CIFAR-10 dataset:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "def load_cifar10():\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "    test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "    train_x = train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "    ret_train_y = to_categorical(train_y,10)\n",
    "    ret_test_y = to_categorical(test_y, 10)\n",
    "    \n",
    "    return (train_x, ret_train_y), (test_x, ret_test_y)\n",
    "\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_cifar10()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 1\n",
    "\n",
    "Explain what the following two  statements do, and where the number \"3072\" came from (2 MARKS):\n",
    "\n",
    "```\n",
    "  train_x = train_x.reshape(train_x.shape[0], 3072) # Question 1\n",
    "  test_x = test_x.reshape(test_x.shape[0], 3072) # Question 1\n",
    "```\n",
    "\n",
    "***The code reshapes the dimensions of the data of both training and test sets. The CIFAR-10 dataset consists of 60,000 32x32x3 (32x32 pixels, and 3 RGB channels) images, so 3x32x32 = 3072 i.e 50,000 training images with 3072 feature columns and 10,000 Test images with 3072 feature columns.***\n",
    "\n",
    "*FOR GRADER: _______ / 2*\n",
    "\n",
    "### 3.2 Building the MLP Classifier\n",
    "\n",
    "In the code box below, create a new fully connected (dense) multilayer perceptron classifier for the CIFAR-10 dataset. To begin with, create a network with one hidden layer of 1024 neurons, using the SGD optimizer. You should output the training and validation accuracy at every epoch, and train for 50 epochs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.1331 - accuracy: 0.3257 - val_loss: 1.1200 - val_accuracy: 0.3736\n",
      "Epoch 2/50\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.1158 - accuracy: 0.3897 - val_loss: 1.1164 - val_accuracy: 0.3874\n",
      "Epoch 3/50\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.1083 - accuracy: 0.4167 - val_loss: 1.1090 - val_accuracy: 0.4080\n",
      "Epoch 4/50\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1031 - accuracy: 0.4315 - val_loss: 1.1082 - val_accuracy: 0.4101\n",
      "Epoch 5/50\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.0989 - accuracy: 0.4457 - val_loss: 1.1008 - val_accuracy: 0.4367\n",
      "Epoch 6/50\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.0952 - accuracy: 0.4589 - val_loss: 1.0965 - val_accuracy: 0.4524\n",
      "Epoch 7/50\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.0919 - accuracy: 0.4698 - val_loss: 1.0943 - val_accuracy: 0.4575\n",
      "Epoch 8/50\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.0890 - accuracy: 0.4786 - val_loss: 1.0960 - val_accuracy: 0.4496\n",
      "Epoch 9/50\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.0862 - accuracy: 0.4864 - val_loss: 1.0915 - val_accuracy: 0.4600\n",
      "Epoch 10/50\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.0837 - accuracy: 0.4954 - val_loss: 1.0913 - val_accuracy: 0.4595\n",
      "Epoch 11/50\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.0813 - accuracy: 0.5031 - val_loss: 1.0899 - val_accuracy: 0.4680\n",
      "Epoch 12/50\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 1.0787 - accuracy: 0.5107 - val_loss: 1.0841 - val_accuracy: 0.4886\n",
      "Epoch 13/50\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.0767 - accuracy: 0.5161 - val_loss: 1.0861 - val_accuracy: 0.4817\n",
      "Epoch 14/50\n",
      "1563/1563 [==============================] - 16s 10ms/step - loss: 1.0744 - accuracy: 0.5239 - val_loss: 1.0871 - val_accuracy: 0.4745\n",
      "Epoch 15/50\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.0724 - accuracy: 0.5325 - val_loss: 1.0840 - val_accuracy: 0.4836\n",
      "Epoch 16/50\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 1.0705 - accuracy: 0.5357 - val_loss: 1.0873 - val_accuracy: 0.4758\n",
      "Epoch 17/50\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0684 - accuracy: 0.5422 - val_loss: 1.0829 - val_accuracy: 0.4862\n",
      "Epoch 18/50\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.0667 - accuracy: 0.5468 - val_loss: 1.0805 - val_accuracy: 0.4976\n",
      "Epoch 19/50\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0645 - accuracy: 0.5542 - val_loss: 1.0809 - val_accuracy: 0.4991\n",
      "Epoch 20/50\n",
      "1563/1563 [==============================] - 23s 14ms/step - loss: 1.0629 - accuracy: 0.5609 - val_loss: 1.0791 - val_accuracy: 0.5019\n",
      "Epoch 21/50\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.0614 - accuracy: 0.5641 - val_loss: 1.0823 - val_accuracy: 0.4907\n",
      "Epoch 22/50\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0596 - accuracy: 0.5691 - val_loss: 1.0779 - val_accuracy: 0.5066\n",
      "Epoch 23/50\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.0584 - accuracy: 0.5727 - val_loss: 1.0783 - val_accuracy: 0.5029\n",
      "Epoch 24/50\n",
      "1563/1563 [==============================] - 19s 12ms/step - loss: 1.0563 - accuracy: 0.5811 - val_loss: 1.0817 - val_accuracy: 0.4956\n",
      "Epoch 25/50\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.0553 - accuracy: 0.5842 - val_loss: 1.0786 - val_accuracy: 0.5027\n",
      "Epoch 26/50\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.0535 - accuracy: 0.5907 - val_loss: 1.0838 - val_accuracy: 0.4852\n",
      "Epoch 27/50\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.0522 - accuracy: 0.5935 - val_loss: 1.0746 - val_accuracy: 0.5166\n",
      "Epoch 28/50\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.0505 - accuracy: 0.5983 - val_loss: 1.0727 - val_accuracy: 0.5220\n",
      "Epoch 29/50\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.0491 - accuracy: 0.6014 - val_loss: 1.0840 - val_accuracy: 0.4878\n",
      "Epoch 30/50\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 1.0477 - accuracy: 0.6060 - val_loss: 1.0738 - val_accuracy: 0.5172\n",
      "Epoch 31/50\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.0463 - accuracy: 0.6105 - val_loss: 1.0779 - val_accuracy: 0.5063\n",
      "Epoch 32/50\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0445 - accuracy: 0.6174 - val_loss: 1.0824 - val_accuracy: 0.4883\n",
      "Epoch 33/50\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.0433 - accuracy: 0.6204 - val_loss: 1.0764 - val_accuracy: 0.5080\n",
      "Epoch 34/50\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.0417 - accuracy: 0.6239 - val_loss: 1.0915 - val_accuracy: 0.4728\n",
      "Epoch 35/50\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0405 - accuracy: 0.6279 - val_loss: 1.0776 - val_accuracy: 0.5076\n",
      "Epoch 36/50\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.0396 - accuracy: 0.6312 - val_loss: 1.0792 - val_accuracy: 0.5062\n",
      "Epoch 37/50\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0382 - accuracy: 0.6350 - val_loss: 1.0959 - val_accuracy: 0.4578\n",
      "Epoch 38/50\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 1.0368 - accuracy: 0.6412 - val_loss: 1.0728 - val_accuracy: 0.5220\n",
      "Epoch 39/50\n",
      "1563/1563 [==============================] - 42s 27ms/step - loss: 1.0357 - accuracy: 0.6427 - val_loss: 1.0730 - val_accuracy: 0.5212\n",
      "Epoch 40/50\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0345 - accuracy: 0.6457 - val_loss: 1.0721 - val_accuracy: 0.5235\n",
      "Epoch 41/50\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.0329 - accuracy: 0.6507 - val_loss: 1.0765 - val_accuracy: 0.5100\n",
      "Epoch 42/50\n",
      "1563/1563 [==============================] - 38s 25ms/step - loss: 1.0320 - accuracy: 0.6534 - val_loss: 1.0738 - val_accuracy: 0.5168\n",
      "Epoch 43/50\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0305 - accuracy: 0.6572 - val_loss: 1.0748 - val_accuracy: 0.5189\n",
      "Epoch 44/50\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0292 - accuracy: 0.6613 - val_loss: 1.0709 - val_accuracy: 0.5275\n",
      "Epoch 45/50\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.0285 - accuracy: 0.6636 - val_loss: 1.0686 - val_accuracy: 0.5369\n",
      "Epoch 46/50\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.0273 - accuracy: 0.6667 - val_loss: 1.0731 - val_accuracy: 0.5208\n",
      "Epoch 47/50\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.0260 - accuracy: 0.6712 - val_loss: 1.0699 - val_accuracy: 0.5295\n",
      "Epoch 48/50\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.0249 - accuracy: 0.6738 - val_loss: 1.0747 - val_accuracy: 0.5198\n",
      "Epoch 49/50\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.0237 - accuracy: 0.6777 - val_loss: 1.0763 - val_accuracy: 0.5097\n",
      "Epoch 50/50\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.0228 - accuracy: 0.6801 - val_loss: 1.0714 - val_accuracy: 0.5256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa82be73a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Write your code to build an MLP with one hidden layer of 1024 neurons,\n",
    "with an SGD optimizer. Train for 50 epochs, and output the training and\n",
    "validation accuracy at each epoch.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create the neural network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(1024,activation = 'relu'))\n",
    "nn.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "sgd = SGD(lr = 0.1, momentum =0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn.compile(loss='squared_hinge', optimizer=sgd,\n",
    "          metrics = 'accuracy')\n",
    "\n",
    "nn.fit(train_x, train_y, shuffle = True, epochs = 50, \n",
    "      validation_data = (test_x, test_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2\n",
    "\n",
    "Complete the following table on the design choices for your MLP \n",
    "(3 MARKS):\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            | SGD         | Specified in question |\n",
    "| # of hidden layers   | 1           | Specified in question |\n",
    "| # of hidden neurons  | 1024        | Specified in question |\n",
    "| Hid layer activation |  relu           |   handles vanishing gradients                    |\n",
    "| # of output neurons  |       10      |        10 classes of images               |\n",
    "| Output activation    |    softmax         |    the problem is multiclass classification, softmax works better               |\n",
    "| learning rate        |    0.1         |      to achieve balance and avoid gradient jumps                 |\n",
    "| momentum             |  0.1           |      accelerates gradient descent in the relevant direction and dampens oscillations                 |\n",
    "| decay                |   0          |             default          |\n",
    "| loss                 |   squared_hinge           |     As classification problem                  |\n",
    "\n",
    "*FOR GRADER:*<br>\n",
    "*Table: ___ / 3* <br>\n",
    "*Code:  ___ / 5* <br>\n",
    "**TOTAL: ____ / 8** <br>\n",
    "\n",
    "#### Question 3:\n",
    "\n",
    "What was your final training accuracy? Validation accuracy? Is there overfitting / underfitting? Explain your answer (5 MARKS)\n",
    "\n",
    "***The final accuracy is 68% and validation accuracy is 53%. Undefitting, as the accuracy increased with the epoch count and can be much more improved.***\n",
    "\n",
    "*FOR GRADER: ______ / 5*\n",
    "\n",
    "### 3.3 Experimenting with the MLP\n",
    "\n",
    "Cut and paste your code from Section 3.2 to the box below (you may need to rename your MLP). Experiment with the number of hidden layers, the number of neurons in each hidden layer, the optimization algorithm, etc. See [Keras Optimizers](https://keras.io/optimizers) for the types of optimizers and their parameters. **Train for 100 epochs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 74s 47ms/step - loss: 1.2600 - accuracy: 0.0999 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 79s 51ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 129s 83ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 107s 68ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 96s 61ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 94s 60ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 94s 60ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 92s 59ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 93s 60ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 100s 64ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 88s 57ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 97s 62ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 108s 69ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 78s 50ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 82s 52ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 86s 55ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 76s 48ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 77s 49ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 77s 50ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 83s 53ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 82s 53ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 84s 54ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 124s 79ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 131s 84ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 104s 67ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 101s 65ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 98s 63ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 100s 64ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 101s 64ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 103s 66ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 98s 63ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 107s 69ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 105s 67ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 143s 91ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 153s 98ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 110s 71ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 105s 67ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 114s 73ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 105s 67ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 161s 103ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 245s 157ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 240s 154ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 123s 79ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 153s 98ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 240s 154ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 222s 142ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 218s 139ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 160s 103ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 197s 126ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 252s 161ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 235s 151ms/step - loss: 1.2600 - accuracy: 0.1000 - val_loss: 1.2600 - val_accuracy: 0.1000\n",
      "Epoch 58/100\n",
      " 443/1563 [=======>......................] - ETA: 2:42 - loss: 1.2597 - accuracy: 0.1007"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/9q84mn0j1nzfg85t9zx5d1480000gn/T/ipykernel_67381/3710248367.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m nn2.fit(train_x, train_y, epochs = 100, \n\u001b[0;32m---> 27\u001b[0;31m       validation_data = (test_x, test_y))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cut and paste your code from Section 3.2 below, then modify it to get\n",
    "much better results than what you had earlier. E.g. increase the number of\n",
    "nodes in the hidden layer, increase the number of hidden layers,\n",
    "change the optimizer, etc. \n",
    "\n",
    "Train for 100 epochs.\n",
    "\n",
    "\"\"\"\n",
    "from keras.optimizers import Adam\n",
    "# Create the neural network\n",
    "nn2 = Sequential()\n",
    "nn2.add(Dense(2048,activation = 'relu'))\n",
    "nn2.add(Dense(1024,activation = 'relu'))\n",
    "nn2.add(Dense(1024,activation = 'relu'))\n",
    "nn2.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Create our optimizer\n",
    "adam = Adam(learning_rate= 0.1)\n",
    "\n",
    "# 'Compile' the network to associate it with a loss function,\n",
    "# an optimizer, and what metrics we want to track\n",
    "nn2.compile(loss='squared_hinge', optimizer=adam,\n",
    "          metrics = 'accuracy')\n",
    "\n",
    "nn2.fit(train_x, train_y, epochs = 100, \n",
    "      validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 4:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (3 MARKS)\n",
    "\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            |    Adam         |    stochastic gradient descent method                   |\n",
    "| # of hidden layers   |     3        |         to improve  Accuracy             |\n",
    "| # neurons(layer1)    |      2048       |      complex structure to improve accuracy                 |\n",
    "| Hid layer1 activation|      relu       |        to handle vanishing graidents                 |\n",
    "| # neurons(layer2)    |     1024        |    complex structure to improve accuracy                   |\n",
    "| Hid layer2 activation|        relu     |         to handle vanishing graidents              |\n",
    "| # of output neurons  |      10     | 10 classes of images             |\n",
    "| Output activation    |   softmax          |     the problem is multiclass classification, softmax works better                    |\n",
    "| learning rate        |    0.1         |      to achieve balance and avoid gradient jumps                 |\n",
    "| momentum             |  0.1           |      accelerates gradient descent in the relevant direction and dampens oscillations                 |\n",
    "| decay                |   0          |             default          |\n",
    "| loss                 |   squared_hinge           |     As classification problem                  |\n",
    "\n",
    "*FOR GRADER:* <br>\n",
    "*TABLE: _____ / 3* <br>\n",
    "*CODE: ______ / 5*<br>\n",
    "\n",
    "***TOTAL: ______ / 8***\n",
    "\n",
    "#### Question 5\n",
    "\n",
    "What is the final training because of the time limitaion and validation accuracy that you obtained after 100 epochs. Is there considerable improvement over Section 3.2? Are there still signs of underfitting or overfitting? Explain your answer (5 MARKS)\n",
    "\n",
    "***I have stopped the model at 58 epochs as the model stuck in local minima and the accuracy was not improving. The accuracy would have been same after 100 epochs. The Neural network doesnt poerform well as per the expectations and the accuracy was 10%. The model is overfitted as the accuray is stuck and local minima has to be avoided.***\n",
    "\n",
    "*FOR GRADER: ______ / 5*\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "Write a short reflection on the practical difficulties of using a dense MLP to classsify images in the CIFAR-10 datasets. (3 MARKS)\n",
    "\n",
    "***The training can be stuck in local minima hence no changes in the accuracy, loss, validation accuracy. The performance with Adam optimizer is very bad, it resulted in 10% accuracy. Adding multliple layers with increased number of neurons makes the network complex and instead of increasing the accuracy, it got overfitted and the accuracy is decreased***\n",
    "\n",
    "*FOR GRADER: _______ /3*\n",
    "\n",
    "----\n",
    "\n",
    "## 4. Creating a CNN for the MNIST Dataset\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in the previous lab. Let's go through each part to see how to do this.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As always we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def load_mnist():\n",
    "    (train_x, train_y),(test_x, test_y) = mnist.load_data()\n",
    "    train_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\n",
    "    test_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\n",
    "\n",
    "    train_x=train_x.astype('float32')\n",
    "    test_x = test_x.astype('float32')\n",
    "    \n",
    "    train_x /= 255.0\n",
    "    test_x /= 255.0\n",
    "        \n",
    "    train_y = to_categorical(train_y, 10)\n",
    "    test_y = to_categorical(test_y, 10)\n",
    "        \n",
    "    return (train_x, train_y), (test_x, test_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Building the CNN\n",
    "\n",
    "We will now build the CNN. Unlike before we will create a function to produce the CNN. We will also look at how to save and load Keras models using \"checkpoints\", particularly \"ModelCheckpoint\" that saves the model each epoch.\n",
    "\n",
    "Let's begin by creating the model. We call os.path.exists to see if a model file exists, and call \"load_model\" if it does. Otherwise we create a new model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model loads a model from a hd5 file.\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'mnist-cnn.hd5'\n",
    "\n",
    "def buildmodel(model_name):\n",
    "    if os.path.exists(model_name):\n",
    "        model = load_model(model_name)                                                                                             \n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(128, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "        model.add(Flatten()) # Question 9\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 7\n",
    "\n",
    "The first layer in our CNN is a 2D convolution kernel, shown here:\n",
    "\n",
    "```\n",
    "        model.add(Conv2D(32, kernel_size=(5,5),\n",
    "        activation='relu',\n",
    "        input_shape=(28, 28, 1), padding='same')) # Question 7\n",
    "```\n",
    "\n",
    "Why is the input_shape set to (28, 28, 1)? What does this mean? What does \"padding = 'same'\" mean? (4 MARKS)\n",
    "\n",
    "***the images are of 28x28 size. (28, 28, 1) means that the image is of length 28, width 28 and has 1 channel i.e color instead of RGB which has 3 channels. Padding is a special form of masking where the masked steps are at the start or the end of a sequence. Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences. \"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right, as is the case in this example (the same logic applies vertically: there may be an extra row of zeros at the bottom).***\n",
    "\n",
    "*FOR GRADER: ______ / 4*\n",
    "\n",
    "#### Question 8\n",
    "\n",
    "The second layer is the MaxPooling2D layer shown below:\n",
    "\n",
    "```\n",
    "        model.add(MaxPooling2D(pool_size=(2,2), strides=2)) # Question 8\n",
    "```\n",
    "\n",
    "What other types of pooling layers are available? What does 'strides = 2' mean? (3 MARKS)\n",
    "\n",
    "***\n",
    "- MaxPooling1D layer\n",
    "- MaxPooling2D layer\n",
    "- MaxPooling3D layer\n",
    "- AveragePooling1D layer\n",
    "- AveragePooling2D layer\n",
    "- AveragePooling3D layer\n",
    "- GlobalMaxPooling1D layer\n",
    "- GlobalMaxPooling2D layer\n",
    "- GlobalMaxPooling3D layer\n",
    "- GlobalAveragePooling1D layer\n",
    "- GlobalAveragePooling2D layer\n",
    "- GlobalAveragePooling3D layer\n",
    "\n",
    "----\n",
    "Strides = 2 means, the filter will move 2 pixels while scanning the image.***\n",
    "\n",
    "*FOR GRADER: _____ / 3*\n",
    "\n",
    "\n",
    "#### Question 9\n",
    "\n",
    "What does the \"Flatten\" layer here do? Why is it needed?\n",
    "\n",
    "```\n",
    "        model.add(Flatten()) # Question 9\n",
    "```\n",
    "\n",
    "***Flattening is converting the data into a 1-dimensional array for inputting it to the next layer. We flatten the output of the convolutional layers to create a single long feature vector. In some architectures, e.g. CNN an image is better processed by a neural network if it is in 1D form rather than 2D. Flattening is used to convert all the resultant 2-Dimensional arrays from pooled feature maps into a single long continuous linear vector. The flattened matrix is fed as input to the fully connected layer to classify the image.***\n",
    "\n",
    "*FOR GRADER: ____ / 2*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "### 4.3 Training the CNN\n",
    "\n",
    "Let's now train the CNN. In this example we introduce the idea of a \"callback\", which is a routine that Keras calls at the end of each epoch. Specifically we look at two callbacks:\n",
    "\n",
    "    1. ModelCheckpoint: When called, Keras saves the model to the specified filename.\n",
    "    \n",
    "    2. EarlyStopping: When called, Keras checks if it should stop the training prematurely.\n",
    "    \n",
    "\n",
    "Let's look at the code to see how training is done, and how callbacks are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def train(model, train_x, train_y, epochs, test_x, test_y, model_name):\n",
    "\n",
    "    model.compile(optimizer=SGD(lr=0.01, momentum=0.7), \n",
    "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    savemodel = ModelCheckpoint(model_name)\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "\n",
    "    print(\"Starting training.\")\n",
    "\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "\n",
    "    print(\"Done. Now evaluating.\")\n",
    "    loss, acc = model.evaluate(x=test_x, y=test_y)\n",
    "    print(\"Test accuracy: %3.2f, loss: %3.2f\"%(acc, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there isn't very much that is unusual going on; we compile the model with our loss function and optimizer, then call fit, and finally evaluate to look at the final accuracy for the test set.  The only thing unusual is the \"callbacks\" parameter here in the fit function call\n",
    "\n",
    "```\n",
    "    model.fit(x=train_x, y=train_y, batch_size=32,\n",
    "    validation_data=(test_x, test_y), shuffle=True,\n",
    "    epochs=epochs, \n",
    "    callbacks=[savemodel, stopmodel])\n",
    "```\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 10.\n",
    "\n",
    "What do the min_delta and patience parameters do in the EarlyStopping callback, as shown below? (2 MARKS)\n",
    "\n",
    "```\n",
    "    stopmodel = EarlyStopping(min_delta=0.001, patience=10) # Question 10\n",
    "```\n",
    "\n",
    "***To Stop training when a monitored metric has stopped improving. min_delta: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "patience: Number of epochs with no improvement after which training will be stopped.***\n",
    "\n",
    "*FOR GRADER: ______ / 2*\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4 Putting it together.\n",
    "\n",
    "Now let's run the code and see how it goes (Note: To save time we are training for only 5 epochs; we should train much longer to get much better results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "Starting training.\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asadtariq/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - ETA: 0s - loss: 0.3462 - accuracy: 0.8907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 195s 103ms/step - loss: 0.3462 - accuracy: 0.8907 - val_loss: 0.0632 - val_accuracy: 0.9791\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9784"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 213s 114ms/step - loss: 0.0695 - accuracy: 0.9784 - val_loss: 0.0476 - val_accuracy: 0.9848\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 193s 103ms/step - loss: 0.0457 - accuracy: 0.9856 - val_loss: 0.0437 - val_accuracy: 0.9861\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 160s 85ms/step - loss: 0.0336 - accuracy: 0.9891 - val_loss: 0.0326 - val_accuracy: 0.9900\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist-cnn.hd5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 158s 84ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.0326 - val_accuracy: 0.9896\n",
      "Done. Now evaluating.\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.0326 - accuracy: 0.9896\n",
      "Test accuracy: 0.99, loss: 0.03\n"
     ]
    }
   ],
   "source": [
    "    (train_x, train_y),(test_x, test_y) = load_mnist()\n",
    "    model = buildmodel(MODEL_NAME)\n",
    "    train(model, train_x, train_y, 5, test_x, test_y, MODEL_NAME)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Question 11.\n",
    "\n",
    "Compare the relative advantages and disadvantages of CNN vs. the Dense MLP that you build in sections 3.2 and 3.3. What makes CNNs better (or worse)? (3 MARKS)\n",
    "\n",
    "***\n",
    "- CNN is mostly used for Image Data, whereas it is better to use MLP on structural data\n",
    "- CNN has less parameters and tries to reduce the dimensions of image whereas in case of MLP number of parameters depends on the data\n",
    "- CNN is complex in nature whereas MLP is relatively simple compared to CNN\n",
    "- CNN uses special Convolution and Pooling Layers whereas MLP is just a network of Neurons\n",
    "- CNN is generally used for huge or bulky data as compared to MLP\n",
    "  ------\n",
    "  \n",
    "  The accuracy is almost 100%, CNN perform very well for image classification***\n",
    "\n",
    "*FOR GRADER: ______ / 3*\n",
    "\n",
    "## 5. Creating a CNN for the CIFAR-10 Dataset\n",
    "\n",
    "Now comes the fun part: Using the example above for creating a CNN for the MNIST dataset, now create a CNN in the box below for the MNIST-10 dataset. At the end of each epoch save the model to a file called \"cifar.hd5\" (note: the .hd5 is added automatically for you).\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 12.\n",
    "\n",
    "Summarize your design in the table below (the actual coding cell comes after this):\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            |  adam           |  stochastic gradient descent method                      |\n",
    "| Input shape          |  (32,32,3)           |    height = 32, width =32, channels = RGB =3                   |\n",
    "| First layer          |    Conv2D(64,(3,3)         |                       |\n",
    "| Second layer         |    Conv2D(64,(3,3)       |                       |\n",
    "| Maxpool layer         |    MaxPooling2D(pool_size=(2,2)      |                       |\n",
    "| third layer         |    Conv2D(128,(3,3)       |                       |\n",
    "| fourth layer         |    Conv2D(128,(3,3)       |                       |\n",
    "| Maxpool layer         |    MaxPooling2D(pool_size=(2,2)      |                       |\n",
    "| fifth layer         |    Conv2D(256,(3,3)     |                       |\n",
    "| Flatten layer         |    Flatten(input_shape=(32,32)     |      to flatten the dimensions                 |\n",
    "| Dense layer  1        |  128           |                       |\n",
    "| Dense layer  2       |        100     |                       |\n",
    "| Dense layer  3        |       80      |                       |\n",
    "| Dense layer  4        |    10         |      Number of outputs                 |\n",
    "\n",
    "\n",
    "*FOR GRADER:* <br>\n",
    "*TABLE: ________ / 3* <br>\n",
    "*CODE: _________/ 7* <br>\n",
    "**TOTAL: _______ / 10** <br>\n",
    "\n",
    "---\n",
    "\n",
    "***TOTAL: _______ / 55***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Shapes of training and test sets are:\n",
      "((50000, 10), array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32))\n",
      "((10000, 10), array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.], dtype=float32))\n",
      "---------------------------------------------------------------------------\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n",
      "Epoch 1/40\n",
      "893/893 [==============================] - 360s 402ms/step - loss: 1.9659 - accuracy: 0.2607 - val_loss: 1.6662 - val_accuracy: 0.3973\n",
      "Epoch 2/40\n",
      "893/893 [==============================] - 357s 400ms/step - loss: 1.5662 - accuracy: 0.4303 - val_loss: 1.4192 - val_accuracy: 0.4852\n",
      "Epoch 3/40\n",
      "399/893 [============>.................] - ETA: 3:58 - loss: 1.4180 - accuracy: 0.4912"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/9q84mn0j1nzfg85t9zx5d1480000gn/T/ipykernel_67381/167884860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2452\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2454\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2456\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1859\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1861\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1862\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    500\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pythonProject/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Write your code for your CNN for the CIFAR-10 dataset here. \n",
    "\n",
    "Note: train_x, train_y, test_x, test_y were changed when we called \n",
    "load_mnist in the previous section. You will now need to call load_cifar10\n",
    "again.\n",
    "\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras import models, layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten , Dense, Activation,Dropout\n",
    "\n",
    "# Loading the dataset\n",
    "Cifar10=keras.datasets.cifar10 \n",
    "(X_train,y_train),(X_test,y_test)= Cifar10.load_data()\n",
    "\n",
    "\n",
    "class_names =['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "\n",
    "# One hot Encoding\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "print('---------------------------------------------------------------------------')\n",
    "# After one hot Encoding\n",
    "print('Shapes of training and test sets are:')\n",
    "print((y_train.shape, y_train[0]))\n",
    "print((y_test.shape, y_test[1]))    \n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------')\n",
    "\n",
    "# Creating Convolution Neural Netword\n",
    "# creating an empty sequential model \n",
    "model=models.Sequential()\n",
    "# Adding CNN Layers in the Neural Network with Relu activation function\n",
    "model.add(layers.Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu'))\n",
    "model.add(layers.Conv2D(64,(3,3),input_shape=(32,32,3),activation='relu'))\n",
    "# Max pooling layer\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "# Second Convolution \n",
    "model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(layers.Conv2D(128,(3,3),activation='relu'))\n",
    "# Max pooling layer\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "# Third convolutional \n",
    "model.add(layers.Conv2D(256,(3,3),activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "#Flatten Layer\n",
    "model.add(layers.Flatten(input_shape=(32,32))) \n",
    "# Classification segment \n",
    "model.add(layers.Dense(128, activation='relu')) \n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(80, activation='relu')) \n",
    "\n",
    "# Adding final output layer to the neural network\n",
    "model.add(layers.Dense(10, activation='softmax')) \n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "#model.summary()\n",
    "\n",
    "# Training the Convolution Neural Network and evaluating the accuracy. \n",
    "X_train2=X_train.reshape(50000,32,32,3)\n",
    "X_test2=X_test.reshape(10000,32,32,3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "model.fit(X_train2,y_train,epochs=40,batch_size=56,verbose=True,validation_data=(X_test2,y_test))\n",
    "\n",
    "train_loss, training_accuracy = model.evaluate(X_train2, y_train)\n",
    "test_loss, test_accuracy = model.evaluate(X_test2, y_test)\n",
    "print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n",
    "print(\"4. Accuracy of Function 4  'Convolution Neural Network' on Training set:\", training_accuracy*100,'%')\n",
    "print('-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*')\n",
    "print(\"5. Accuracy of Function 4  'Convolution Neural Network' on Test set:\", test_accuracy*100,'%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('3.9.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "055b62fcaec9821674a26809055da6bc29fe87d96b4c426e8bdfbe57b9f21334"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
