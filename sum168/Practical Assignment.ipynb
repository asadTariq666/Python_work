{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names: ['Mustafa Al-Tamimi', 'How Jong Lai'] \n",
    "## Studentnumbers: ['12717916', '12426083']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NuJ0879WJF0E",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "Names: ['Mustafa Al-Tamimi', 'How Jong Lai'] \n",
    "Studentnumbers: ['12717916', '12426083']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PHjzAhvgBlo7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\n",
      "zsh:1: command not found: gdown\n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "# Dataset with 10K instances\n",
    "! gdown \"https://drive.google.com/uc?id=1cj-CzkY6QZUe42ky64GI5CSSg7-K40N5\"\n",
    "# Dataset with 100k instances\n",
    "! gdown \"https://drive.google.com/uc?id=1MbWGXLawE3VTxP1XgNpj8uEo1VHPq12B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SlHJBPXQJF0K",
    "nbgrader": {
     "checksum": "86f687db575cf409d54ac5e91e6dd861",
     "grade": false,
     "grade_id": "cell-1979a89473cf7ece",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 1: Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "id": "wKpbQMIQJF0K",
    "nbgrader": {
     "checksum": "89914f38bf3510543caaef66d0f14169",
     "grade": false,
     "grade_id": "cell-700f4a41782fa412",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Applied Machine learning\n",
    "## Practical Assignment 2\n",
    "\n",
    "### Important Notes:\n",
    "1. Submit through **Canvas** before 11:59pm on Tuesday, May 17, 2022\n",
    "2. No late homework will be accepted\n",
    "3. This is a group-of-two assignment\n",
    "4. The submitted file should be in ipynb format\n",
    "5. The assignment is worth it 10 points\n",
    "6. For questions, please use the discussion part of Canvas (English only!)\n",
    "7. The indication **optional** means that the question is optional; you won't lose any points if you do not do that part of the assignment, nor will you gain if you do it.\n",
    "\n",
    "### Software:\n",
    "We will be using Python programming language throughout this course. Further we will be using:\n",
    "+ IPython Notebooks (as an environment)\n",
    "+ Numpy\n",
    "+ Pandas\n",
    "+ Scikit-learn\n",
    "\n",
    "### Background:\n",
    "\n",
    "This practical assignment will be covering logistic regression, neural networks, support vector machines and evaluation of classifiers. \n",
    "\n",
    "For the assignment, please download a dataset on Load Defaults. You are provided with two datasets:\n",
    "1. [Dataset](https://drive.google.com/open?id=1cj-CzkY6QZUe42ky64GI5CSSg7-K40N5) with 10,000 instances \n",
    "2. [Dataset](https://drive.google.com/open?id=1MbWGXLawE3VTxP1XgNpj8uEo1VHPq12B) with 100,000 instances\n",
    "\n",
    "In principle you should work on the second, larger dataset, but if you face scaling computational issues then better work with the first, smaller dataset.\n",
    "\n",
    "This data corresponds to a set of financial transactions associated with individuals. The data has been standardized, de-trended, and anonymized. You are provided with thousands of observations and nearly 800 features. Each observation (instance) is independent from the previous. \n",
    "\n",
    "For each observation, it was recorded whether a default was triggered (i.e. whether an individual could not pay back the loan). In case of a default, the loss for the bank was measured. This quantity lies between 0 and 100. If the loan did not default, the loss for the bank was 0. You are asked to predict whether a loan will default or not for each individual in the test set.\n",
    "\n",
    "Missing feature values have been kept as is, so that the competing teams can really use the maximum data available, implementing a strategy to fill the gaps if desired. Consider all variables continuous, even though some variables may be categorical (e.g. f776 and f777).\n",
    "\n",
    "The goal of the machine learning algorithm will be to predict whether a loan will default, given a set of features. For privacy reasons the feature names are not provided.\n",
    "\n",
    "**Important Note**: This second assignment is not as instructive as the first assignment. The first assignment guided you step-by-step through all the preprocessing, training-validation-testing setup, etc. This assignment does not do so, but it leaves it up to you to decide how to use the data and design your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "k63PQb_KJF0L",
    "nbgrader": {
     "checksum": "cdc9100b2eb32f795318f88531439408",
     "grade": false,
     "grade_id": "cell-3a36fe2e430fa9ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "df = pd.read_csv('loan_default_100K.csv', sep=\",\", header=0, dtype=np.float64)\n",
    "\n",
    "# Drop the observations that contain missing values\n",
    "dfn = df.dropna(0, how='any')\n",
    "\n",
    "# Consider only a handful of features to start with; you can extend to the full set later on.\n",
    "X = dfn.loc[:,'f700':'f750'].values\n",
    "\n",
    "\n",
    "# Generate the labels; if 'loss' is zero the this indicates the negative class, class 0, i.e. no default;\n",
    "# if 'loss' is possitive this indicates the positive class, class 1, i.e. there is a loan default;\n",
    "#y = [ bool(y) for y in dfn.loc[:,'loss'].values ]\n",
    "dfn['loss'].mask(dfn['loss'] == 0.0, 0, inplace=True)\n",
    "dfn['loss'].mask(dfn['loss'] != 0.0, 1, inplace=True)\n",
    "y = np.array(dfn[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    44137\n",
       "1.0     4954\n",
       "Name: loss, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn['loss'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into train, validation, and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.20,random_state=42, stratify= y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>...</th>\n",
       "      <th>f770</th>\n",
       "      <th>f771</th>\n",
       "      <th>f772</th>\n",
       "      <th>f773</th>\n",
       "      <th>f774</th>\n",
       "      <th>f775</th>\n",
       "      <th>f776</th>\n",
       "      <th>f777</th>\n",
       "      <th>f778</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.500080</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>83607.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1527.0</td>\n",
       "      <td>127.76</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>-1.73</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.502749</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79124.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>122.72</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>6.11</td>\n",
       "      <td>-3.82</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>-0.5399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.985674</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>13026.0</td>\n",
       "      <td>4565.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>126.36</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.06</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>3.77</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>-0.6732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.385778</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79244.0</td>\n",
       "      <td>6597.0</td>\n",
       "      <td>3592.0</td>\n",
       "      <td>127.19</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.45</td>\n",
       "      <td>-3.26</td>\n",
       "      <td>2.56</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>-0.7220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.745471</td>\n",
       "      <td>2900.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>78920.0</td>\n",
       "      <td>3058.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>123.89</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.02</td>\n",
       "      <td>-1.35</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.2601</td>\n",
       "      <td>0.7132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99991</th>\n",
       "      <td>99992.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.185661</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>80421.0</td>\n",
       "      <td>4766.0</td>\n",
       "      <td>1152.0</td>\n",
       "      <td>123.56</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.95</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>2.34</td>\n",
       "      <td>0.2714</td>\n",
       "      <td>-0.7475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99992</th>\n",
       "      <td>99993.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.793214</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8563.0</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>3088.0</td>\n",
       "      <td>128.30</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.09</td>\n",
       "      <td>-5.97</td>\n",
       "      <td>4.60</td>\n",
       "      <td>0.2901</td>\n",
       "      <td>0.6060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99993</th>\n",
       "      <td>99994.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.794861</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>78935.0</td>\n",
       "      <td>615.0</td>\n",
       "      <td>1872.0</td>\n",
       "      <td>124.51</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>5.76</td>\n",
       "      <td>-4.21</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.2969</td>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99994</th>\n",
       "      <td>99995.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.690208</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>83726.0</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>128.92</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>-3.41</td>\n",
       "      <td>2.46</td>\n",
       "      <td>0.2883</td>\n",
       "      <td>-0.2677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>99998.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.667488</td>\n",
       "      <td>1300.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13317.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>124.57</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.47</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>2.49</td>\n",
       "      <td>0.3074</td>\n",
       "      <td>-0.6631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49091 rows × 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     f1    f2        f3      f4   f5       f6      f7      f8  \\\n",
       "2          3.0  126.0  10.0  0.500080  1100.0  3.0  83607.0  1800.0  1527.0   \n",
       "4          5.0  109.0   9.0  0.502749  2900.0  4.0  79124.0    89.0   491.0   \n",
       "6          7.0  121.0   9.0  0.985674  2900.0  4.0  13026.0  4565.0   263.0   \n",
       "7          8.0  128.0   9.0  0.385778  2900.0  4.0  79244.0  6597.0  3592.0   \n",
       "8          9.0  126.0   9.0  0.745471  2900.0  4.0  78920.0  3058.0   112.0   \n",
       "...        ...    ...   ...       ...     ...  ...      ...     ...     ...   \n",
       "99991  99992.0  127.0  10.0  0.185661  1300.0  7.0  80421.0  4766.0  1152.0   \n",
       "99992  99993.0  131.0  10.0  0.793214  1300.0  7.0   8563.0  1062.0  3088.0   \n",
       "99993  99994.0  129.0  10.0  0.794861  1300.0  7.0  78935.0   615.0  1872.0   \n",
       "99994  99995.0  133.0  10.0  0.690208  1300.0  7.0  83726.0  1522.0   968.0   \n",
       "99997  99998.0  125.0  10.0  0.667488  1300.0  7.0  13317.0   219.0  1955.0   \n",
       "\n",
       "           f9  ...  f770  f771  f772  f773    f774    f775  f776  f777  f778  \\\n",
       "2      127.76  ...  13.0  2.89 -1.73  1.04  0.2521  0.7258   1.0   0.0   5.0   \n",
       "4      122.72  ...  26.0  6.11 -3.82  2.51  0.2282 -0.5399   0.0   0.0   5.0   \n",
       "6      126.36  ...  23.0  7.06 -4.99  3.77  0.2458 -0.6732   0.0   0.0   5.0   \n",
       "7      127.19  ...  17.0  4.45 -3.26  2.56  0.2947 -0.7220   0.0   0.0   5.0   \n",
       "8      123.89  ...   7.0  2.02 -1.35  0.95  0.2601  0.7132   0.0   0.0   5.0   \n",
       "...       ...  ...   ...   ...   ...   ...     ...     ...   ...   ...   ...   \n",
       "99991  123.56  ...  24.0  4.95 -3.29  2.34  0.2714 -0.7475   0.0   0.0  31.0   \n",
       "99992  128.30  ...  24.0  8.09 -5.97  4.60  0.2901  0.6060   0.0   0.0  31.0   \n",
       "99993  124.51  ...  19.0  5.76 -4.21  3.28  0.2969  0.3042   0.0   0.0  31.0   \n",
       "99994  128.92  ...  20.0  5.00 -3.41  2.46  0.2883 -0.2677   0.0   0.0  31.0   \n",
       "99997  124.57  ...  18.0  4.47 -3.24  2.49  0.3074 -0.6631   0.0   0.0  31.0   \n",
       "\n",
       "       loss  \n",
       "2       0.0  \n",
       "4       0.0  \n",
       "6       0.0  \n",
       "7       1.0  \n",
       "8       0.0  \n",
       "...     ...  \n",
       "99991   0.0  \n",
       "99992   0.0  \n",
       "99993   0.0  \n",
       "99994   0.0  \n",
       "99997   0.0  \n",
       "\n",
       "[49091 rows x 771 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0LKlXxS3JF0L",
    "nbgrader": {
     "checksum": "ff730342a91fefa515c3117b502aa292",
     "grade": false,
     "grade_id": "cell-735be096b0f642ef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 2: Evaluation measures (2pts)\n",
    "In what follows you should implement a number of evaluation measures. You need to implement these from scratch, meaning that it is not allowed to call any scikit-learn function, or any other API function that implements the method for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4yvWH4cgJF0L",
    "nbgrader": {
     "checksum": "cdadfd696f9d159f0242cad28b7f41ef",
     "grade": false,
     "grade_id": "cell-a127e8413f617d64",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that produces the contigency matrix, i.e. True Positives, False Positives, True Negatives, False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "id": "294-dY4BJF0M",
    "nbgrader": {
     "checksum": "bd892db60c7761bace1ca06b9361a045",
     "grade": false,
     "grade_id": "cell-aab0f82d6a21bea5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def contigency_matrix(true_y, predicted_y):\n",
    "    # YOUR CODE HERE, Create TP, FP, TN, FN\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(predicted_y)): \n",
    "        if true_y[i]==predicted_y[i]==1:\n",
    "            TP += 1\n",
    "        if predicted_y[i]==1 and true_y[i]!=predicted_y[i]:\n",
    "            FP += 1\n",
    "        if true_y[i]==predicted_y[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_y[i]==0 and true_y[i]!=predicted_y[i]:\n",
    "            FN += 1\n",
    "\n",
    "    #return(TP, FP, TN, FN)\n",
    "    \n",
    "    # Make sure your output fits the following format:\n",
    "    matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39272, 9819)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  99,  892],\n",
       "       [7936,  892]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contigency_matrix(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cV3BfX8jJF0M",
    "nbgrader": {
     "checksum": "b2f901219adca9d441082d3fcf702e6b",
     "grade": false,
     "grade_id": "cell-b3c52de1970e361e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes accuracy (without using any built-in accuracy function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "id": "DK2lCwiNJF0M",
    "nbgrader": {
     "checksum": "f6705877fef7cd74d89832d32a8536a0",
     "grade": false,
     "grade_id": "cell-2e0cc734628dd4c2",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(true_y, predicted_y):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(predicted_y)): \n",
    "        if true_y[i]==predicted_y[i]==1:\n",
    "            TP += 1\n",
    "        if predicted_y[i]==1 and true_y[i]!=predicted_y[i]:\n",
    "            FP += 1\n",
    "        if true_y[i]==predicted_y[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_y[i]==0 and true_y[i]!=predicted_y[i]:\n",
    "            FN += 1\n",
    "    \n",
    "    matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    \n",
    "    upper = TP + TN\n",
    "    lower = TP + TN + FN + FP\n",
    "    if(lower!=0):\n",
    "        accuracy = upper/lower\n",
    "        return accuracy\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8183114370098789"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4LznpIdTJF0M",
    "nbgrader": {
     "checksum": "4b3798ba720235065011afa2736346a7",
     "grade": false,
     "grade_id": "cell-d045e95a552112ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes precision for one class (without using any built-in precision function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "id": "QC8SeWOIJF0M",
    "nbgrader": {
     "checksum": "40f4c7470d6073739cff4934761469c8",
     "grade": false,
     "grade_id": "cell-a403be8ac0ee7af0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def precision(true_y, predicted_y):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(predicted_y)): \n",
    "        if true_y[i]==predicted_y[i]==1:\n",
    "            TP += 1\n",
    "        if predicted_y[i]==1 and true_y[i]!=predicted_y[i]:\n",
    "            FP += 1\n",
    "        if true_y[i]==predicted_y[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_y[i]==0 and true_y[i]!=predicted_y[i]:\n",
    "            FN += 1\n",
    "    \n",
    "    matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    \n",
    "    upper = TP\n",
    "    lower = TP + FP\n",
    "    if lower!=0:\n",
    "        precision = upper/lower\n",
    "        return precision\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09989909182643794"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_D7OYnK4JF0N",
    "nbgrader": {
     "checksum": "6dbe4298af11247615077250b24d3f59",
     "grade": false,
     "grade_id": "cell-4822b32d0cedb0e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes recall for one class (without using any built-in recall function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "id": "X08ucQRFJF0N",
    "nbgrader": {
     "checksum": "cf41435f0b5003e31ae61340d3188bde",
     "grade": false,
     "grade_id": "cell-075963e37ff41c66",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def recall(true_y, predicted_y):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(predicted_y)): \n",
    "        if true_y[i]==predicted_y[i]==1:\n",
    "            TP += 1\n",
    "        if predicted_y[i]==1 and true_y[i]!=predicted_y[i]:\n",
    "            FP += 1\n",
    "        if true_y[i]==predicted_y[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_y[i]==0 and true_y[i]!=predicted_y[i]:\n",
    "            FN += 1\n",
    "    \n",
    "    matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    \n",
    "    upper = TP\n",
    "    lower = TP + FN\n",
    "    if(lower!=0):\n",
    "        recall = upper/lower\n",
    "        return recall\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09989909182643794"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yE78OP9tJF0N",
    "nbgrader": {
     "checksum": "c0fa7903c66407fa515b5c4c5e12b266",
     "grade": false,
     "grade_id": "cell-f0c5ff30db6fc1b0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Implement a function that computes f1 for one class(without using any built-in f1 function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "id": "FB-f5OO1JF0N",
    "nbgrader": {
     "checksum": "0b8095ecb0d05692b4a57cc17eff026d",
     "grade": false,
     "grade_id": "cell-bcc41b9d876ee5d4",
     "locked": false,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def f1(true_y, predicted_y):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(predicted_y)): \n",
    "        if true_y[i]==predicted_y[i]==1:\n",
    "            TP += 1\n",
    "        if predicted_y[i]==1 and true_y[i]!=predicted_y[i]:\n",
    "            FP += 1\n",
    "        if true_y[i]==predicted_y[i]==0:\n",
    "            TN += 1\n",
    "        if predicted_y[i]==0 and true_y[i]!=predicted_y[i]:\n",
    "            FN += 1\n",
    "    \n",
    "    matrix = np.array(([TP, FP], [TN, FN]))\n",
    "    \n",
    "    upper = TP\n",
    "    lower = (TP + 0.5*(FP+FN))\n",
    "    if(lower!=0):\n",
    "        f1 = upper/lower\n",
    "        return f1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09989909182643794"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jO5VhQLWJF0N",
    "nbgrader": {
     "checksum": "e0d9785e1e45e2179c318aec035059ad",
     "grade": false,
     "grade_id": "cell-c21fd73cbce64a50",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part 3: Algorithms\n",
    "Compare the performance of Logistic Regression, SVMs and Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "UWXjVAenJF0N",
    "nbgrader": {
     "checksum": "af0943a520201c48990805ce5cb2cb7c",
     "grade": false,
     "grade_id": "cell-7ea6f44ccf633c76",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Logistic Regression (Lecture 3) (2pts)\n",
    "\n",
    "+ Train and test a logistic regression model\n",
    "    + Construct a table with each row being a different value of the regularization parameter and each column the aforementioned measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train and test logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# instantiate the model\n",
    "logreg1 = LogisticRegression(solver='liblinear',penalty='l1', random_state=0)\n",
    "logreg2 = LogisticRegression(solver='liblinear',penalty='l2',random_state = 0)\n",
    "logreg3 = LogisticRegression(solver='newton-cg',penalty='l2', random_state=0)\n",
    "logreg4 = LogisticRegression(solver='lbfgs',penalty='l2',random_state = 0)\n",
    "logreg5 = LogisticRegression(solver='saga',penalty='elasticnet',random_state = 0,l1_ratio=1)\n",
    "logreg6 = LogisticRegression(solver='saga',penalty='l1', random_state=0,l1_ratio=1)\n",
    "logreg7 = LogisticRegression(solver='saga',penalty='l2',random_state = 0,l1_ratio=1)\n",
    "# fit the model\n",
    "logreg1.fit(X_train, y_train)\n",
    "logreg2.fit(X_train, y_train)\n",
    "logreg3.fit(X_train, y_train)\n",
    "logreg4.fit(X_train, y_train)\n",
    "logreg5.fit(X_train, y_train)\n",
    "logreg6.fit(X_train, y_train)\n",
    "logreg7.fit(X_train, y_train)\n",
    "\n",
    "#Predictions based on models\n",
    "y_pred_test1 = logreg1.predict(X_test)\n",
    "y_pred_test2 = logreg2.predict(X_test)\n",
    "y_pred_test3 = logreg3.predict(X_test)\n",
    "y_pred_test4 = logreg4.predict(X_test)\n",
    "y_pred_test5 = logreg5.predict(X_test)\n",
    "y_pred_test6 = logreg6.predict(X_test)\n",
    "y_pred_test7 = logreg7.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8991\n",
      "Model Precision score: 0.0000\n",
      "Model Recall score: 0.0000\n",
      "Model f1 score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 1 - solver: liblinear, penalty: l1\n",
    "\n",
    "accuracy_1 = accuracy(y_test, y_pred_test1)\n",
    "precision_1 = precision(y_test, y_pred_test1)\n",
    "recall_1 =recall(y_test, y_pred_test1)\n",
    "f1_1 = f1(y_test, y_pred_test1)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test1)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test1)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test1)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8991\n",
      "Model Precision score: 0.0000\n",
      "Model Recall score: 0.0000\n",
      "Model f1 score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 2 - solver: liblinear, penalty: l2\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test2)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test2)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test2)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test2)))\n",
    "\n",
    "accuracy_2 = accuracy(y_test, y_pred_test2)\n",
    "precision_2 = precision(y_test, y_pred_test2)\n",
    "recall_2 =recall(y_test, y_pred_test2)\n",
    "f1_2 = f1(y_test, y_pred_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8992\n",
      "Model Precision score: 1.0000\n",
      "Model Recall score: 0.0010\n",
      "Model f1 score: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 3 - solver: newton-cg, penalty: l2\n",
    "accuracy_3 = accuracy(y_test, y_pred_test3)\n",
    "precision_3 = precision(y_test, y_pred_test3)\n",
    "recall_3 =recall(y_test, y_pred_test3)\n",
    "f1_3 = f1(y_test, y_pred_test3)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test3)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test3)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test3)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8991\n",
      "Model Precision score: 0.0000\n",
      "Model Recall score: 0.0000\n",
      "Model f1 score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 4 - solver: lbfgs, penalty: l2\n",
    "accuracy_4 = accuracy(y_test, y_pred_test4)\n",
    "precision_4 = precision(y_test, y_pred_test4)\n",
    "recall_4 =recall(y_test, y_pred_test4)\n",
    "f1_4 = f1(y_test, y_pred_test4)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test4)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test4)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test4)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8991\n",
      "Model Precision score: 0.5000\n",
      "Model Recall score: 0.0010\n",
      "Model f1 score: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 5 - solver: saga, penalty: elasticnet\n",
    "accuracy_5 = accuracy(y_test, y_pred_test5)\n",
    "precision_5 = precision(y_test, y_pred_test5)\n",
    "recall_5 =recall(y_test, y_pred_test5)\n",
    "f1_5 = f1(y_test, y_pred_test5)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test5)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test5)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test5)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8991\n",
      "Model Precision score: 0.5000\n",
      "Model Recall score: 0.0010\n",
      "Model f1 score: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 6 - solver: saga, penalty: l1\n",
    "accuracy_6 = accuracy(y_test, y_pred_test6)\n",
    "precision_6 = precision(y_test, y_pred_test6)\n",
    "recall_6 =recall(y_test, y_pred_test6)\n",
    "f1_6 = f1(y_test, y_pred_test6)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test6)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test6)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test6)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.8991\n",
      "Model Precision score: 0.5000\n",
      "Model Recall score: 0.0010\n",
      "Model f1 score: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model # 7 - solver: saga, penalty: l2\n",
    "accuracy_7 = accuracy(y_test, y_pred_test7)\n",
    "precision_7 = precision(y_test, y_pred_test7)\n",
    "recall_7 =recall(y_test, y_pred_test7)\n",
    "f1_7 = f1(y_test, y_pred_test7)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test, y_pred_test7)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test, y_pred_test7)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test, y_pred_test7)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test, y_pred_test7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a table with each row being a different value of the regularization parameter and each column the aforementioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "vnnZ6T1MJF0O",
    "nbgrader": {
     "checksum": "bb4b24cb9b3769517aafa02dce6321d4",
     "grade": true,
     "grade_id": "cell-eea85664ef370cd5",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with different regularization parameters of Logistic Regression:\n",
      "\n",
      "      Solver     Penalty  Accuracy  Precision    Recall  F1-Score\n",
      "0  liblinear          l1  0.899073        0.0  0.000000  0.000000\n",
      "1  liblinear          l2  0.899073        0.0  0.000000  0.000000\n",
      "2  newton-cg          l2  0.899175        1.0  0.001009  0.002016\n",
      "3      lbfgs          l2  0.899073        0.0  0.000000  0.000000\n",
      "4       saga  elasticnet  0.899073        0.5  0.001009  0.002014\n",
      "5       saga          l1  0.899073        0.5  0.001009  0.002014\n",
      "6       saga          l2  0.899073        0.5  0.001009  0.002014\n"
     ]
    }
   ],
   "source": [
    "# Construct table \n",
    "# Construct a table with each row being a different value of the regularization parameter and each column the aforementioned\n",
    "cols=['Solver','Penalty','Accuracy','Precision','Recall','F1-Score']\n",
    "\n",
    "Logistic_Regression_Table = pd.DataFrame(columns=cols)\n",
    "# List of series with same Index as datframe\n",
    "listOfSeries = [pd.Series(['liblinear', 'l1', accuracy_1,precision_1,recall_1,f1_1], index=Logistic_Regression_Table.columns ) ,\n",
    "                pd.Series(['liblinear', 'l2', accuracy_2,precision_2,recall_2,f1_2], index=Logistic_Regression_Table.columns ) ,\n",
    "                pd.Series(['newton-cg', 'l2', accuracy_3,precision_3,recall_3,f1_3], index=Logistic_Regression_Table.columns ) ,\n",
    "                pd.Series(['lbfgs', 'l2', accuracy_4,precision_4,recall_4,f1_4], index=Logistic_Regression_Table.columns ) ,\n",
    "                pd.Series(['saga', 'elasticnet', accuracy_5,precision_5,recall_5,f1_5], index=Logistic_Regression_Table.columns ) ,\n",
    "                pd.Series(['saga', 'l1', accuracy_6,precision_6,recall_6,f1_6], index=Logistic_Regression_Table.columns ) ,\n",
    "                pd.Series(['saga', 'l2', accuracy_7,precision_7,recall_7,f1_7], index=Logistic_Regression_Table.columns ) \n",
    "                ]\n",
    "Logistic_Regression_Table = Logistic_Regression_Table.append(  listOfSeries,\n",
    "                        ignore_index=True)\n",
    "print('Table with different regularization parameters of Logistic Regression:\\n') \n",
    "print(Logistic_Regression_Table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991, 8828)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y_test == 1), np.count_nonzero(y_test == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 9819)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y_pred_test1 == 1), np.count_nonzero(y_pred_test1 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 9817)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(y_pred_test7 == 1), np.count_nonzero(y_pred_test7 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Optimal Model\n",
    "Optimal Model is with following parameters:\n",
    "\n",
    "Solver: newton-cg\n",
    "\n",
    "Penalty: l2\n",
    "\n",
    "This model returns similar accuracy but clearly better Precision, Recall and f1-score as compared to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report performance of Optimal model\n",
    "All performance measures except for Precision are almost similar. This model with **Solver: newton-cg and Penalty: l2** has clearly perfect Precision in predicting the test label. The low values of Recall, Precision and F1-score are because of uneven distribution of data and a large number of contributing features, which affects the prediction of True Negative, False positives and False Negatives. \n",
    "\n",
    "Logistic Regression models occasionally have problem in predicting Rare events, In our training set, we have observed that not even 10% of the training set has label 1, the model is probably going to have an elevated level of type 2 error - which fails to identify a significant predictor of Y even if it is \"really\" related to Y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1s in test class:  991 \n",
      "Total 0s in test class:  8828\n",
      "total 1s in Predicted test class:  0 \n",
      "Total 0s in Predictedtest class:  9819\n"
     ]
    }
   ],
   "source": [
    "print('total 1s in test class: ',np.count_nonzero(y_test == 1),'\\nTotal 0s in test class: ', np.count_nonzero(y_test == 0))\n",
    "print('total 1s in Predicted test class: ',np.count_nonzero(y_pred_test1 == 1),'\\nTotal 0s in Predictedtest class: ', np.count_nonzero(y_pred_test1 == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1s in training class:  3963 \n",
      "Total 0s in training class:  35309\n"
     ]
    }
   ],
   "source": [
    "print('total 1s in training class: ',np.count_nonzero(y_train == 1),'\\nTotal 0s in training class: ', np.count_nonzero(y_train == 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KNigDLF2JF0O",
    "nbgrader": {
     "checksum": "0d74b83486a9bcdcbdf539376bd56e52",
     "grade": false,
     "grade_id": "cell-f57d340c8ac3f3b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "* Explain what you observe regarding the positive class; i.e. the performance of the algorithm in predicting defaults. Explain why is this happening.\n",
    "\n",
    "Only 1 model predicted 1 positive value for the entire dataset. \n",
    "The reason for this is uneven distribution of data (Positve instances are not even 10% of the total dataset) and a large number of contributing features(778).\n",
    "\n",
    "Logistic Regression models occasionally have problem in predicting Rare events, In our training set, we have observed that not even 10% of the training set has label 1, the model is probably going to have an elevated level of type 2 error - which fails to identify a significant predictor of Y even if it is \"really\" related to Y.\n",
    "\n",
    "**Solution**:  Downsizing the dataset for even distribution.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT1gcqt_JF0P"
   },
   "source": [
    "# The last few questions below are not optional!\n",
    "If you did not finish the optional downsampling, just go through with the data created before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5DL60WojJF0Q",
    "nbgrader": {
     "checksum": "88788580e9ea0cc74560a55c6231466b",
     "grade": false,
     "grade_id": "cell-d7e21719abffa28b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### SVMs (Lecture 4) (2pts)\n",
    "\n",
    "+ Train and test a Support Vector Machine model\n",
    "    + Construct a table with each row being a different configuration of the SVM algorithm (play with the regularization parameter, and the kernel function – use linear, poly, and rbf) and each column the evaluation measures\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "k63PQb_KJF0L",
    "nbgrader": {
     "checksum": "cdc9100b2eb32f795318f88531439408",
     "grade": false,
     "grade_id": "cell-3a36fe2e430fa9ab",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Reading dataset with 10k values\n",
    "\n",
    "df = pd.read_csv('loan_default_10K.csv', sep=\",\", header=0, dtype=np.float64)\n",
    "\n",
    "# Drop the observations that contain missing values\n",
    "dfn = df.dropna(0, how='any')\n",
    "\n",
    "# Consider only a handful of features to start with; you can extend to the full set later on.\n",
    "X = dfn.loc[:,'f1':'f778'].values\n",
    "\n",
    "\n",
    "# Generate the labels; if 'loss' is zero the this indicates the negative class, class 0, i.e. no default;\n",
    "# if 'loss' is possitive this indicates the positive class, class 1, i.e. there is a loan default;\n",
    "#y = [ bool(y) for y in dfn.loc[:,'loss'].values ]\n",
    "dfn['loss'].mask(dfn['loss'] == 0.0, 0, inplace=True)\n",
    "dfn['loss'].mask(dfn['loss'] != 0.0, 1, inplace=True)\n",
    "y = np.array(dfn[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    3952\n",
       "1.0     415\n",
       "Name: loss, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfn['loss'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class count\n",
    "count_class_0, count_class_1 = dfn.loss.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "df_class_0 = dfn[dfn['loss'] == 0]\n",
    "df_class_1 = dfn[dfn['loss'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under sampling:\n",
      "0.0    415\n",
      "1.0    415\n",
      "Name: loss, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Count (loss)'}>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "print('Under sampling:')\n",
    "print(df_test_under.loss.value_counts())\n",
    "\n",
    "df_test_under.loss.value_counts().plot(kind='bar', title='Count (loss)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    415\n",
       "1.0    415\n",
       "Name: loss, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_under['loss'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider only a handful of features to start with; you can extend to the full set later on.\n",
    "X = df_test_under.loc[:,'f1':'f778'].values\n",
    "\n",
    "\n",
    "# Generate the labels; if 'loss' is zero the this indicates the negative class, class 0, i.e. no default;\n",
    "# if 'loss' is possitive this indicates the positive class, class 1, i.e. there is a loan default;\n",
    "#y = [ bool(y) for y in dfn.loc[:,'loss'].values ]\n",
    "df_test_under['loss'].mask(df_test_under['loss'] == 0.0, 0, inplace=True)\n",
    "df_test_under['loss'].mask(df_test_under['loss'] != 0.0, 1, inplace=True)\n",
    "y = np.array(df_test_under[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into train, validation, and test.\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25,random_state=112, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((622, 769), (622,), (208, 769))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape,y_train1.shape,X_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train1),type(y_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "id": "Ac5mNYD3JF0P",
    "nbgrader": {
     "checksum": "167eed2eaafde3d4951aef9059e06a58",
     "grade": true,
     "grade_id": "cell-c6f93c6c6f633c47",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.svm import SVC\n",
    "# instantiate classifier with default hyperparameters\n",
    "svc1=SVC(C=100)\n",
    "# instantiate classifier with linear kernel \n",
    "svc2=SVC(kernel='linear')\n",
    "# instantiate classifier with polynomial kernel\n",
    "svc3=SVC(kernel='poly',C=100)\n",
    "\n",
    "\n",
    "# fit classifiers to training set\n",
    "svc1.fit(X_train1, y_train1)\n",
    "\n",
    "\n",
    "#Predictions based on models\n",
    "y_pred_svc1 = svc1.predict(X_test1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc2.fit(X_train1, y_train1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc2 = svc2.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, kernel='poly')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc3.fit(X_train1, y_train1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svc3 = svc3.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.5144\n",
      "Model Precision score: 0.5882\n",
      "Model Recall score: 0.0962\n",
      "Model f1 score: 0.1653\n",
      "Model accuracy score: 0.5000\n",
      "Model Precision score: 0.5000\n",
      "Model Recall score: 1.0000\n",
      "Model f1 score: 0.6667\n",
      "Model accuracy score: 0.5144\n",
      "Model Precision score: 1.0000\n",
      "Model Recall score: 0.0288\n",
      "Model f1 score: 0.0561\n"
     ]
    }
   ],
   "source": [
    "#  classifier with default hyperparameters\n",
    "\n",
    "accuracy_1_svm = accuracy(y_test1, y_pred_svc1)\n",
    "precision_1_svm = precision(y_test1, y_pred_svc1)\n",
    "recall_1_svm =recall(y_test1, y_pred_svc1)\n",
    "f1_1_svm = f1(y_test1, y_pred_svc1)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test1, y_pred_svc1)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test1, y_pred_svc1)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test1, y_pred_svc1)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test1, y_pred_svc1)))\n",
    "\n",
    "\n",
    "#  classifier with linear kernel\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test1, y_pred_svc2)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test1, y_pred_svc2)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test1, y_pred_svc2)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test1, y_pred_svc2)))\n",
    "\n",
    "accuracy_2_svm = accuracy(y_test1, y_pred_svc2)\n",
    "precision_2_svm = precision(y_test1, y_pred_svc2)\n",
    "recall_2_svm =recall(y_test1, y_pred_svc2)\n",
    "f1_2_svm = f1(y_test1, y_pred_svc2)\n",
    "\n",
    "#  classifier with poly kernel\n",
    "accuracy_3_svm = accuracy(y_test1, y_pred_svc3)\n",
    "precision_3_svm = precision(y_test1, y_pred_svc3)\n",
    "recall_3_svm =recall(y_test1, y_pred_svc3)\n",
    "f1_3_svm = f1(y_test1, y_pred_svc3)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test1, y_pred_svc3)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test1, y_pred_svc3)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test1, y_pred_svc3)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test1, y_pred_svc3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with different regularization parameters of SVM:\n",
      "\n",
      "   kernel  Accuracy  Precision    Recall  F1-Score\n",
      "0     rbf  0.514423   0.588235  0.096154  0.165289\n",
      "1  linear  0.500000   0.500000  1.000000  0.666667\n",
      "2    poly  0.514423   1.000000  0.028846  0.056075\n"
     ]
    }
   ],
   "source": [
    "# Construct table \n",
    "#Construct a table with each row being a different configuration of the \n",
    "# SVM algorithm (play with the regularization parameter, and the kernel function – use linear, poly, and rbf)\n",
    "cols_svm=['kernel','Accuracy','Precision','Recall','F1-Score']\n",
    "\n",
    "SVM_Table = pd.DataFrame(columns=cols_svm)\n",
    "# List of series with same Index as datframe\n",
    "listOfSeriesSVM = [pd.Series(['rbf', accuracy_1_svm,precision_1_svm,recall_1_svm,f1_1_svm], index=SVM_Table.columns ) ,\n",
    "                pd.Series(['linear', accuracy_2_svm,precision_2_svm,recall_2_svm,f1_2_svm], index=SVM_Table.columns ) ,\n",
    "                pd.Series(['poly', accuracy_3_svm,precision_3_svm,recall_3_svm,f1_3_svm], index=SVM_Table.columns )  \n",
    "                ]\n",
    "SVM_Table = SVM_Table.append(  listOfSeriesSVM,\n",
    "                        ignore_index=True)\n",
    "print('Table with different regularization parameters of SVM:\\n') \n",
    "print(SVM_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "hte-UpsCJF0Q",
    "nbgrader": {
     "checksum": "2412fc015a280623635f3bf03e3fde9e",
     "grade": true,
     "grade_id": "cell-b4ae750d1154f837",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Report**</span>\n",
    "\n",
    "The Support Vector Classifiers were taking too long to train and predict on the complete dataset and the results were very poor. The primary reason for this poor performance was because of uneven distribution of data. Target class distribution was observed to be 10%-90%, to counter this problem of poor sampling, we downsized our data set to match number of 0's to Positive labels. After downsizing, we splitted the data in to training and testing datasets. \n",
    "\n",
    "We have trained and tested our classifiers by considering all features. The classifiers we used are,\n",
    "1. Classifier with RBF kernel\n",
    "2. Classifier with Linear kernel\n",
    "3. Classifier with polynomial kernel\n",
    "\n",
    "The accuracy of all models are nearly similar, but the Precision for 1st Classifier with rbf kernel performed slightly worse in Precision Perspective. The 2nd classifier with linear Kernel performed better in Recall and F1-score perspectives. \n",
    "\n",
    "But we will consider ***3rd classifier with poly kernel as better classifier*** because of its slightly better accuracy and balanced Precision, Recall and F1-scores. Accuracy is comparatively better and precision is perfect, Recall and F1 scores are relatively low but its a trade off between Precision and recall in our model. \n",
    "\n",
    "The overall performance in accuracy is not that good because of larger number of Contributing features i.e. 770+ features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YI7doqzLJF0P",
    "nbgrader": {
     "checksum": "fdac93da6191896d74c7346d2e875a84",
     "grade": false,
     "grade_id": "cell-4dc578274728380b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "##### Neural Network (Lecture 5) (2pts)\n",
    "\n",
    "+ Train and test a Neural Network model\n",
    "    + Construct a table with each row being a different configuration of the network (play with the number of hidden layers, the number of neurons in each layer, and the activation function) and each column the evaluation measures\n",
    "    + Report the performance of at least three different configurations\n",
    "    + Explain your findings and select the optimal model\n",
    "    + Justify your choice of different paramteres and architectures\n",
    "    + Report the performance of the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208, 769), (622, 769), (622,), (208,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test1.shape,X_train1.shape,y_train1.shape,y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "YWgD6iFmJF0Q",
    "nbgrader": {
     "checksum": "e4d7a215763017f7111a6162d214ed5e",
     "grade": true,
     "grade_id": "cell-7009d775455986ad",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# Create a classifier: Hidden Layers:5, activation: tanh, solver:sgd\n",
    "classifier_5 = MLPClassifier(hidden_layer_sizes=(50,),activation='tanh',solver='sgd', alpha=0.0001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier_5.fit(X_train1, y_train1)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "# Predicting the Test set results\n",
    "y_pred_cl5 = classifier_5.predict(X_test1)\n",
    "\n",
    "\n",
    "# Create a classifier: Hidden Layers:10, activation: relu, solver:adam\n",
    "classifier_10 = MLPClassifier(hidden_layer_sizes=(100,),activation='relu',solver='adam', alpha=0.0001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier_10.fit(X_train1, y_train1)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "# Predicting the Test set results\n",
    "y_pred_cl10 = classifier_10.predict(X_test1)\n",
    "\n",
    "\n",
    "# Create a classifier: Hidden Layers:1000, activation: identity, solver:lbfgs\n",
    "classifier_20 = MLPClassifier(hidden_layer_sizes=(1000,),activation='identity',solver='lbfgs', alpha=0.0001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier_20.fit(X_train1, y_train1)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "# Predicting the Test set results\n",
    "y_pred_cl20 = classifier_20.predict(X_test1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier: Hidden Layers:1000, activation: relu, solver:adam\n",
    "classifier_10a = MLPClassifier(hidden_layer_sizes=(1000,),activation='relu',solver='adam', alpha=0.0001)\n",
    "\n",
    "# We learn the digits on the first half of the digits\n",
    "classifier_10a.fit(X_train1, y_train1)\n",
    "\n",
    "# Now predict the value of the digit on the second half:\n",
    "# Predicting the Test set results\n",
    "y_pred_cl10a = classifier_10a.predict(X_test1)\n",
    "\n",
    "accuracy_2_nna = accuracy(y_test1, y_pred_cl10a)\n",
    "precision_2_nna = precision(y_test1, y_pred_cl10a)\n",
    "recall_2_nna =recall(y_test1, y_pred_cl10a)\n",
    "f1_2_nna = f1(y_test1, y_pred_cl10a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score: 0.4760\n",
      "Model Precision score: 0.4752\n",
      "Model Recall score: 0.4615\n",
      "Model f1 score: 0.4683\n",
      "Model accuracy score: 0.5000\n",
      "Model Precision score: 0.5000\n",
      "Model Recall score: 1.0000\n",
      "Model f1 score: 0.6667\n",
      "Model accuracy score: 0.5000\n",
      "Model Precision score: 0.5000\n",
      "Model Recall score: 1.0000\n",
      "Model f1 score: 0.6667\n"
     ]
    }
   ],
   "source": [
    "#  classifier: Hidden Layers:50, activation: tanh, solver:sgd\n",
    "\n",
    "accuracy_1_nn = accuracy(y_test1, y_pred_cl5)\n",
    "precision_1_nn = precision(y_test1, y_pred_cl5)\n",
    "recall_1_nn =recall(y_test1, y_pred_cl5)\n",
    "f1_1_nn = f1(y_test1, y_pred_cl5)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test1, y_pred_cl5)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test1, y_pred_cl5)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test1, y_pred_cl5)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test1, y_pred_cl5)))\n",
    "\n",
    "\n",
    "# classifier: Hidden Layers:100, activation: relu, solver:adam\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test1, y_pred_cl10)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test1, y_pred_cl10)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test1, y_pred_cl10)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test1, y_pred_cl10)))\n",
    "\n",
    "accuracy_2_nn = accuracy(y_test1, y_pred_cl10)\n",
    "precision_2_nn = precision(y_test1, y_pred_cl10)\n",
    "recall_2_nn =recall(y_test1, y_pred_cl10)\n",
    "f1_2_nn = f1(y_test1, y_pred_cl10)\n",
    "\n",
    "#  classifier: Hidden Layers:1000, activation: identity, solver:lbfgs\n",
    "accuracy_3_nn = accuracy(y_test1, y_pred_cl20)\n",
    "precision_3_nn = precision(y_test1, y_pred_cl20)\n",
    "recall_3_nn =recall(y_test1, y_pred_cl20)\n",
    "f1_3_nn = f1(y_test1, y_pred_cl20)\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy(y_test1, y_pred_cl20)))\n",
    "print('Model Precision score: {0:0.4f}'. format(precision(y_test1, y_pred_cl20)))\n",
    "print('Model Recall score: {0:0.4f}'. format(recall(y_test1, y_pred_cl20)))\n",
    "print('Model f1 score: {0:0.4f}'. format(f1(y_test1, y_pred_cl20)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with different regularization parameters of Neural Network:\n",
      "\n",
      "  Hidden layers activation solver  Accuracy  Precision    Recall  F1-Score\n",
      "0            50       tanh    sgd  0.475962   0.475248  0.461538  0.468293\n",
      "1           100       relu   adam  0.500000   0.500000  1.000000  0.666667\n",
      "2          1000   identity  lbfgs  0.500000   0.500000  1.000000  0.666667\n",
      "3          1000       relu   adam  0.471154   0.485000  0.932692  0.638158\n"
     ]
    }
   ],
   "source": [
    "# Construct table \n",
    "# Construct a table with each row being a different configuration of the network \n",
    "# (play with the number of hidden layers, the number of neurons in each layer, and the activation function\n",
    "cols_nn=['Hidden layers','activation','solver','Accuracy','Precision','Recall','F1-Score']\n",
    "\n",
    "NeuralNetwork_Table = pd.DataFrame(columns=cols_nn)\n",
    "# List of series with same Index as datframe\n",
    "listOfSeriesNN = [pd.Series([50, 'tanh','sgd',accuracy_1_nn,precision_1_nn,recall_1_nn,f1_1_nn], index=NeuralNetwork_Table.columns ) ,\n",
    "                pd.Series([100,'relu', 'adam',accuracy_2_nn,precision_2_nn,recall_2_nn,f1_2_nn], index=NeuralNetwork_Table.columns ) ,\n",
    "                pd.Series([1000, 'identity','lbfgs', accuracy_3_nn,precision_3_nn,recall_3_nn,f1_3_nn], index=NeuralNetwork_Table.columns ),\n",
    "                pd.Series([1000,'relu', 'adam',accuracy_2_nna,precision_2_nna,recall_2_nna,f1_2_nna], index=NeuralNetwork_Table.columns )  \n",
    "                ]\n",
    "NeuralNetwork_Table = NeuralNetwork_Table.append(  listOfSeriesNN,\n",
    "                        ignore_index=True)\n",
    "print('Table with different regularization parameters of Neural Network:\\n') \n",
    "print(NeuralNetwork_Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "Pw5mJ1z1JF0Q",
    "nbgrader": {
     "checksum": "36cda951bae6da0700d3f6202d0f1a89",
     "grade": true,
     "grade_id": "cell-42c8bf3a6a671fef",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Report**</span>\n",
    "\n",
    "\n",
    "The Neural Networks were taking too long to train and predict on the complete dataset and the results were very poor. The primary reason for this poor performance was because of uneven distribution of data. Target class distribution was observed to be 10%-90%, to counter this problem of poor sampling, we downsized our data set to match number of 0's to Positive labels. After downsizing, we splitted the data in to training and testing datasets. \n",
    "\n",
    "We have trained and tested our classifiers by considering all features. The classifiers we used are,\n",
    "1. Classifier with Hidden Layers:50, activation: tanh, solver:sgd\n",
    "2. Classifier with Hidden Layers:100, activation: relu, solver:adam\n",
    "3. Classifier with Hidden Layers:1000, activation: identity, solver:lbfgs\n",
    "4. Classifier with Hidden Layers:1000, activation: relu, solver:adam\n",
    "\n",
    "The accuracy of all models are nearly similar, but the Precision for 3rd Classifier with lbfgs Solver performed  worst in Precision, Recall and F1-score perspectives. \n",
    "\n",
    "We will consider ***2nd classifier with with Hidden Layers:100, activation: relu, solver:adam*** because of its slightly better accuracy and balanced Precision and F1-scores. Recall score is perfect for this model. We have made another classifier with same regularization parameters with increased numbr of layers but the performance of both Relu models with 100 and 1000 hidden layers respectively is identical. So we will consider Relu model with lower number of layers because of lesser computation utilization. \n",
    "\n",
    "The overall performance in accuracy is not that good because of larger number of Contributing features i.e. 770+ features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OIgJSXMTJF0Q",
    "nbgrader": {
     "checksum": "baabfebe46559e2d055cef7ceae38b0e",
     "grade": false,
     "grade_id": "cell-23280b034d299667",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "#### Compare Algorithms (2pts)\n",
    "* Plot the Precision-Recall curves for the best model for each one of the above algorithms, Logistic Regression, Neural Nets, and SVM.\n",
    "    * Use the precision_recall_curve from scikit-learn\n",
    "* Explain your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/9q84mn0j1nzfg85t9zx5d1480000gn/T/ipykernel_31476/1747011726.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m display = PrecisionRecallDisplay.from_estimator(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlogreg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Logisitic Regression\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0max_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logisitic Regression Precision-Recall curve\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logreg3' is not defined"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# Logistic Regression Precision Recall Display\n",
    "\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    logreg3, X_test, y_test, name=\"Logisitic Regression\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"Logisitic Regression Precision-Recall curve\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "8kRDcyLuJF0Q",
    "nbgrader": {
     "checksum": "b17597ab95452fa3d4259cb4d2bee6c2",
     "grade": true,
     "grade_id": "cell-d6967e3b3e3dbe7a",
     "locked": false,
     "points": 1.5,
     "schema_version": 1,
     "solution": true
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM Precision Recall Display\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    svc3, X_test1, y_test1, name=\"SVC\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"SVC Precision-Recall curve\")\n",
    "\n",
    "\n",
    "#Neural Network Precision Recall Display\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "display = PrecisionRecallDisplay.from_estimator(\n",
    "    classifier_10, X_test1, y_test1, name=\"Neural Network\"\n",
    ")\n",
    "_ = display.ax_.set_title(\"Neural Network Precision-Recall curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "id": "_vShAj6OJF0Q",
    "nbgrader": {
     "checksum": "58ca34644cf139f5987905e2058ea0b3",
     "grade": true,
     "grade_id": "cell-b91319845bc74219",
     "locked": false,
     "points": 0.5,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "<span style=\"color:blue\">**Report**</span>\n",
    "\n",
    "Precision-Recall is an important measure of performance measure of prediction when classes are very inbalanced. As in our case, our classes are very much inbalanced (10%-90%).\n",
    "\n",
    "- Recall: Shows how many true relevant results are provided.\n",
    "- Precision: Shows the Result Relevancy.\n",
    "\n",
    "The precision recall curve indicates the trade-off between recall and precision to a different boundary. The high point at the bottom of the curve represents both high recall and high precision, where high precision is associated with a low false positive level, and high recall is related to a low false standard. High scores on both show that the editor returns correct results (high precision), as well as restoring most of all positive results (high recall).\n",
    "\n",
    "A system with high recall but low precision returns many results, but many of its predicted labels are not accurate in comparison with training labels. A system with high precision but low recall is just the opposite, yielding very few results, but most of its predicted labels are good compared to training labels. Proper precision and high recall will return many results, with all the results labeled correctly.\n",
    "\n",
    "In our models, **Support Vector Classifier**  performed better than other classifiers because of a better Precision Recall curve. The area under the curve for this model is grater as compared to other models. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "collapsed_sections": [],
   "name": "Practical Assignment 2 - Student.ipynb ",
   "provenance": [
    {
     "file_id": "17ja4GM2Ci0MjQjO3RjTNBS0wcnLBpBIr",
     "timestamp": 1618862623296
    },
    {
     "file_id": "1tO5F-kPHZPpMGLNuPjfNqd3rXU1Lkmn1",
     "timestamp": 1618842712618
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
